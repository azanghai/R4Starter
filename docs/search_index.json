[["index.html", "R语言入门 欢迎 学习目标 学习形式 关于R语言课程 课程是如何开展的？ 我可以参加课程嘛？ 联系我们", " R语言入门 文庭 2024-02-03 欢迎 欢迎来到R语言入门教程，本教程是在曼彻斯特大学Andrew Stewart教授开设的Advanced Data Skills, Open Science and Reproducibility课程基础上（已获得授权），进行了一些修改和增添后进行呈现的。关于修改和增添的详细来源可以查看每章最后的“引用”部分。同时Professor Andrew Stewart 的课程原件（英文）可在这个GitHub存储库查看。 在跟随Andrew Stewart教授学习的过程中我被他无私的开源精神和强大的学术能力打动，进而萌生了翻译他开源课程的想法，于是便有了本课程。在这里，同样也向所有开源软件、教程的设计者和创造者们致敬。 学习目标 通过系列的课程学习，希望大家可以掌握R语言的基本语法，并且能够自行编写R语言代码和撰写R Markdown文档完成数据的预处理、分析、呈现、汇报系列步骤，同时掌握初级的统计分析方法。 学习形式 关于R语言课程 您所看到的当前R语言课程是获准进行翻译的，并由C公益支持，旨在提供R语言的入门和分析教程。由于翻译的仓促和准备匆忙，如果您发现教程中存在任何问题，欢迎联系 xuwenting@cyouthchange.org 进行反馈。 课程是如何开展的？ 本课程分为自主学习和线上工作坊两个阶段，并按章节学习。在自主学习阶段，需要您自行阅读对应章节的内容并进行实践，随后参加线上工作坊，在线上工作坊阶段，会相互讨论解决学习中出现的问题，撰写代码完成练习并提供补充知识，技巧等内容。 我可以参加课程嘛？ 当然！该课程向所有人开放。目前该课程在上线测试阶段，如果您想参加系统的课程学习和线上工作坊，请发送包含您联系方式的邮件至 assessment@cyouthchange.org 并注明参加R语言课程学习，我们将为您安排最近的开班日期并邀请您进群。 请注意 为保证学习和上线测试的质量，我们需要您确保可以参加全部的课程（每周约1-2次，每次约1.5h左右），并且在线上工作坊中可以全程打开摄像头（这对大家的交流沟通十分重要），同时您还需要为每一节课程填写课程反馈。以帮助改进课程内容。 联系我们 欢迎通过 assessment@cyouthchange.org 联系我们 "],["在开始之前.html", "Chapter 1 在开始之前 开放研究和可重复性 如何进行可重复的研究 研究功效（Experimental Power） 总结 帮助我们改进本节课程", " Chapter 1 在开始之前 开放研究和可重复性 在本节课中，我们将介绍开放研究的关键概念，并讨论心理、生物医学和生命科学领域所谓的”复制危机”，它导致了开放研究运动，我们还将讨论在您自己的研究中采用可重复的研究实践的重要性，并介绍可以将其纳入自己的研究工作流程中的各种工具和流程，以便您进行可重复的研究。 开放研究 大家可以观看下面的视频了解开放研究的简史 （Youtube视频）和对应幻灯片（存储在谷歌云盘服务上），也可直接阅读下方由AI生成的总结。 视频内容主要讲述了心理学和生物医学科学领域中所谓的”复制危机”，以及研究领域随着这一危机的出现而发生的变化。文章指出，许多研究结果在复制实验中无法得到验证，甚至存在虚假阳性的情况。这是因为研究者在进行研究时存在一些问题，比如选择性地报告数据、删除异常值、进行多次统计分析等，这些行为被称为”p-hacking”。为了解决这些问题，一些组织和机构开始致力于推动开放研究和透明度，改变学术奖励制度，提供培训，教授研究者如何进行更好的研究。此外，还提到了一些解决这一问题的方法，如增加实验的样本量、改变学术奖励机制、加强对开放研究和透明度的教育培训等。 Highlights  “复制危机”：心理学和生物医学科学领域存在许多研究结果无法复制的问题。  “p-hacking”：研究者存在选择性报告数据、删除异常值、进行多次统计分析等行为，导致虚假阳性结果。  开放研究和透明度：一些组织和机构致力于推动开放研究和透明度，改善研究质量。  学术奖励机制：需要改变学术奖励机制，从数量导向转向质量导向。  培训和教育：研究者需要接受培训和教育，学习开放研究和透明度的实践方法。 同时，我们还推荐阅读由Ionnidis（2005）撰写的论文（可选），该论文可以说开启了围绕可重复性的对话，并在过去几年中对心理学和整个生物医学科学研究产生了深刻的影响。同样的，在2019年Dorothy Bishop的文章准确的捕获了几年后发生的情况。 如何进行可重复的研究 习惯了旧的研究方式的研究人员面临的最大挑战之一是他们认为自己不具备采用开放和可重复的研究实践的知识或技术技能。但这并不难！在运行实验之前，您可以预先注册您的假设，以便在分析和编写结果时，可以证明您的预测确实是在数据收集之前做出的。您还可以将您的研究数据与您的代码一起开放（并且公平），以便其他人可以重新创建您的分析。并通过在预印本服务器（例如 PsyArXiv 或 bioRxiv或OSF）上发布您的研究文章）在提交给期刊之前，您将研究结果向所有人公开。采用R 等开源软件还意味着您产生的任何研究成果都可以由能够访问您的数据和代码的其他人重新产生。使用开放工具使我们能够生成开放（且可重用）数据和代码的原则是课程背后的基本理念。观看下面的视频（Youtube平台）和对应幻灯片（存储在谷歌云盘服务上）或者下面由AI生成的总结。在视频中讨论了如何采用开放且可重复的研究实践。 在这个演讲中，介绍了一些你可以在自己的工作流程中采用的研究实践，这将最终使你的研究更加开放和可复制。研究领域的快速变化突显了软件和计算基础设施的广泛采用，这些都是研究人员将其研究变得更加开放和可复制的方式。政治家们也对此表示关注，一些大学也高度参与开放研究实践。许多资助机构现在要求研究人员在获得资助时采用开放研究实践。开放研究不仅仅是指开放数据和代码，还包括预注册研究假设、建立可复制的数据处理和分析流程、使用开放科学方法等。采用开放和可复制的研究实践不仅使研究结果更具可信度，还有助于促进大规模合作和提高研究人员的效率。 Highlights  研究领域的快速变化突显了软件和计算基础设施的广泛采用，以使研究更加开放和可复制。  政治家们也对开放研究实践表示关注，一些大学也高度参与其中。  许多资助机构要求研究人员在获得资助时采用开放研究实践。  开放研究不仅仅是指开放数据和代码，还包括预注册研究假设、建立可复制的数据处理和分析流程、使用开放科学方法等。  采用开放和可复制的研究实践不仅使研究结果更具可信度，还有助于促进大规模合作和提高研究人员的效率。 如果你需要一些指南 可以阅读这一篇论文，Crüwell et al. (2019)介绍了可以采取哪些措施来使您自己的研究更加开放。 对于心理学而言，Gerald Haeffel提到了心理学需要更多地关注理论发展（并鼓励发表反驳理论的结果）。 研究功效（Experimental Power） 在这一部分我们将讨论研究的功效（Power），以及为什么它而很重要。“可重复性危机（replication crisis）”揭示的一个观点是，研究往往对感兴趣的效应大小缺乏足够的统计功效（也就是说,即使效应存在,你的实验也不太可能找到它）。很多问题都源于研究人员没有花足够时间考虑他们研究设计的统计功效问题。即使统计功效不足的研究确实发现了感兴趣的效应,效应大小本身也会被高估(从而为未来基于这一错误效应大小估计进行功效计算的工作带来问题)。 一个解决方案是在实验设计过程中进行数据模拟。使用R有许多方法可以做到这一点,CRAN(the Comprehensive R Archive Network)上有几个包提供了用于不同设计的数据模拟功能。 介绍 理解统计 假设有一个检验，有95%的概率可以正确诊断出一个人没有患有某种疾病。 （特异性= 0.95）。同时，这个检验还拥有80%的准确率，可以正确诊断出某人患有某种疾病（敏感性=0.8）。最后，这种疾病的实际患病率为1%。 现在看来这种检验如何？95%的准确率可以诊断出一个人没有患病，80%的准确率可以诊断出一个人患有某种疾病。看起来还不错？让我们继续看下去。 在10000个人中，当患病率为1%时。有100个人患有某种疾病。由于检验的敏感性为0.8，在100名患者中，80%被检测出来了（80人真阳性），20%没有被检测出来（20人假阴性）。在剩余9900名实际没有患病的人群中，由于特异性为0.95。95%的人被正确诊断为未患病（9405名真阴性），但是5%的人却被错误诊断为阳性（495名假阳性）。也就是说，检测结果报告575人（80+495）患有某种疾病，但是其中有495名假阳性，即86%的人被误诊为患病了（假阳性）(Colquhoun 2014), 现在你是否对统计学有了一些新的理解？ 传统虚无假设显著性检验（NHST）基础 对于有两个实验组的差异，通常来说： 零假设（H0）：这两个实验组之间没有统计学上的显著差异; 实验假设（H1）：两个实验组之间存在统计学上显著的差异。 通常情况下，我们拒绝H0。当然，这是建立在比较两个实验组统计测试结果为p &lt; 0.05（这是研究人员通常选择的alpha (α)水平）。 什么是统计的显著性？ 假设我们做了这样一个实验，随机各分配一组人接受治疗和安慰剂，我们测量每种治疗的平均反应,并希望知道观察到的两种平均值之间的差异是否真实(不为零),或者它是否可能仅为随机产生。 如果我们的到了两种治疗的差异并得到显著性测试的结果是p = 0.05,我们可以做出以下陈述: 如果治疗实际上没有效应(如果真实的平均值之间的差异为零),则等于或大于此次试验实际观察值的概率将是p = 0.05。换句话说,若真实情况下无差异，产生至少和我们此次实验一样大的差异的概率是5%。 因5%的概率较低，我们不认为我们得到的效应是随机的，因此认为两种治疗的差异是真实的。 但是，许多研究者甚至无法正确定义p值的含义，这很令人担忧，2016年,美国统计协会（American Statistical Association， ASA）不得不发表一篇文章,提醒研究人员p值可以得出什么结论,什么不能。 5%从何而来 通常情况下，我们认为当P&lt;0.05的时候，就可以帮助我们作出统计学上的判断。0.05似乎就像“真理”一样，刻在每一位使用统计学的人脑海里，被认为是一种规定，但其实并不十分正确。从另一个角度来说，0.05似乎是因为符合我们内心对偶然和“真实”事物进行划分的分界线。 和你的朋友做一个游戏：你掷一枚硬币，如果正面朝上，你将付给他们一元钱；如果是反面朝上，他们将付给你一元钱。你不断来掷，说来奇怪，一只是反面朝上。在你的朋友认为你在作弊之前，你能掷多少次？ 通常情况下（不那么愤世嫉俗的话），在连续四次反面朝上后，许多人开始变得怀疑，在连续五次反面朝上后，剩下的人也会变得怀疑。让我们计算一下概率，一次反面朝上的概率是50%，两次反面朝上的概率是25%（50%*50%），三次反面朝上的概率是12.5%（50%*50%*50%），四次反面朝上的概率是6.25%（50%*50%*50%*50%），五次反面朝上的概率是3.125%（50%*50%*50%*50%*50%），所以5%正好介于四次与五次之间。 ASA关于p值的原则 p值可以指示数据与特定统计模型的不兼容程度。 p值不测量所研究的假设为真的概率,也不测量数据仅由随机机会产生的概率。 科学结论和商业或政策决策不应仅根据p值是否通过特定阈值来做出。 正确的推理需要充分报告和透明度。 p值或统计显著性不能衡量效应的大小或结果的重要性。 p值本身不能很好地衡量关于模型或假设的证据。(Wasserstein and Lazar 2016) I类和II类错误（Type I and Type II Errors） 在α水平为0.05的情况下,我们有5%的可能性错误地拒绝零假设(H0)。错误地拒绝H0称为I类错误(即认为我们发现了一个差异,但实际上没有)。还有II类错误,它涉及在存在差异的情况下未能找到差异。你以前学到的大多数内容可能都是关于如何避免I类错误。 插入梗图 控制II类错误的发生和控制I类错误的发生同样重要，II类错误发生的概率也被称为β。造成II类错误发生的概率（没有找到本存在的差异）和实验的功效（experimental power)有关。同时，对于任一实验，功效（Power） = 1 - β。 实验的功效真的那么重要吗？ Cohen (1992)描述了为什么实验功效这么重要（以及当实验功效不足时会发生什么）。低功效的研究发现一个真实效应的可能性更低，同时有问题的研究实践（QRPs）还会有可能在不存在显著效应时显示一个更高的效应存在。在报告中提及了他当时进行的对《Journal of Abnormal and Social Psychology》1960年卷的回顾研究结果，以及Sedlmeier和Gigerenzer（1989）对同一杂志1984年卷的回顾研究结果。 在1960年，《Journal of Abnormal and Social Psychology》中报告的实验，用于检测中等效应大小的平均功效为0.48。而到了1984年，这个数值降至0.25（换句话说，即使效应存在，只有25%的机会找到效应！）。 Button等人（2013年）在《自然·神经科学评论》（Nature Reviews Neuroscience）中指出，小样本量削弱了神经科学研究的可靠性。Nord等人（2017年）在《神经科学杂志》中强调了神经科学研究中力量的广泛异质性。 Cohen’s d（标准化平均偏差、鉴别力指数） Cohen’s d 是一种用于衡量效应大小的统计指标。在统计学中，效应大小是指独立变量对因变量的影响程度。在这里的上下文中，它与实验的“功效”（Power）有关。功效（1-β）与以下因素相关： 样本量（N）：样本量越大，发现实际效应的概率越高，功效越大。 效应大小：效应大小越大，检测到效应的可能性越高，从而功效越大。 α水平：这是犯第一类错误（错误地拒绝零假设）的概率。常用的α水平是0.05。 Cohen (1992) 提出，一个合理的功效目标应该是大约0.8。这意味着有80%的概率检测到实际存在的效应。功效为0.8（β为0.20,即犯第二类错误的概率），结合α为0.05，意味着与各自错误相关的风险比是4:1（β:α）。 在衡量效应大小时，Cohen’s d 和 Pearson’s r是常用的指标。对于不同的效应大小，它们的值分别为： Table 1.1: Cohen’s d 和 Pearson’s r 的效应大小 Effect.Size Small.Effect Medium.Effect Large.Effect Cohen’s d 0.2 0.5 0.8 Pearson’s r 0.1 0.3 0.5 等效性检验（Equivalence Testing） 关于等效性检验，推荐阅读Daniël Lakens等人的心理学研究中的等效性检验：教程（Equivalence Testing for Psychological Research: A Tutorial） 数据模拟 在这里，我们向大家推荐几个用于数据模拟的包（package）。 faux: Simulation for Factorial Designs SIMR 当没有实际效应时 Figure 1.1: 当没有实际效应时不同模拟次数的p值分布情况 正如上图1.1所示，当没有实际效应时，对应100,000次模拟实验（N=25），p值得分布较为均匀，这时我们误判p值小于0.05的概率较小。但是当模拟实验的次数增加时（图中为50次和10次），我们可以发现p值得分布变得不均匀，这时我们误判p值小于0.05的概率较大。 真实效应并不一直可以复制 观察下图1.2，我们可以发现，当真实效应为0.2时，对应100,000次模拟实验（N=50），假设 p值小于 0.05 的 alpha 水平，N=50 将为我们提供约 30% 的功效，这意味着我们有70%的概率会错过效应（即使它存在）。 Figure 1.2: d = 0.2时，N=50下进行100,000次模拟实验的P值分布情况 当N=200时，假设 p 值小于 0.05 的 alpha 水平，N=200 将为我们提供约 80% 的功效，这意味着我们只有 20% 的概率会错过找到效应（即使它存在）。如图1.3 Figure 1.3: d = 0.2时，N=200下进行100,000次模拟实验的P值分布情况 样本量对研究结果的影响 当我们从总体中抽取样本时，数据中总会存在一定程度的变异性，这称为抽样误差。这种变异性可能会导致错误的结论，尤其是当我们使用小样本时。 正如之前的图中，在比较“简单”和“复杂”条件的情况下，如果我们从同一总体中抽取小样本（例如，N=20），则样本之间存在差异的可能性更高，这些差异仅仅是由于随机偶然。这可能会导致我们得出两种条件之间存在实际差异的结论，而实际上并不存在。 这就是可疑的研究实践（QRP）发挥作用的地方。QRP是一组技术，经常用于增加在没有实际效果的情况下找到统计显著结果的可能性。一些常见的QRP包括： 数据挖掘：这涉及以多种方式分析数据，以便找到统计显著的结果，即使结果没有意义或不可复现。 p-hacking：这涉及操纵数据或统计分析以获得更有利的 p 值。 HARKing：这涉及在结果已知之后进行假设，而不是在收集数据之前。 QRP 可能会导致误导性结论，并损害科学研究的可信度。重要的是研究人员要意识到 QRP，并避免在自己的工作中使用它们。 总结 统计功效很重要 - 低功效的实验是浪费时间（通常是您自己的！）、金钱和资源，例如实验室等。 低功效的实验加上可疑的研究实践（QRPs）和发表偏见会导致文献中充斥着错误的研究论文。 您要测试的科学理论/模型需要允许您确定感兴趣的最小效应量 - 正是这个最小效应量使您需要为实验提供动力。 即使在高功效研究中（例如，80%）有时也无法找到效果，即使效果存在，并且由于NHST导致可能缺乏对效果的证据，这并不等同于拥有该效果没有的证据。当我们的测试不显着时，我们无法得出效应不存在的结论只是我们没有支持结论它存在的证据。 帮助我们改进本节课程 此处应插入文本和反馈二维码或问卷链接 References "],["开源软件和r语言.html", "Chapter 2 开源软件和R语言 开源软件 开始使用 R 和 RStudio Desktop 入门 开始你的第一个脚本 帮助我们改进本节课程", " Chapter 2 开源软件和R语言 开源软件 概述 如果您想进行开放且可重复的研究，您应该在工作流程中使用开源软件。使用专有软件进行的研究不能轻易被其他人复制。 开源软件是被许可可以自由修改、重新混合和改进的软件。它通常是免费使用的，并以开放交流、协作参与、快速原型制作、透明度、精英管理和面向社区的开发等原则为中心。开源运动始于 20 世纪 80 年代初，部分原因是打印机的出现，并在该十年以Richard Stallman建立的自由软件基金会的形式进一步发展。20 世纪 90 年代末，开源倡议发起，旨在提高人们对开源软件的认识和采用，并在开源实践社区之间架起桥梁。 开源软件由许多人制作，并在符合 OSD 的许可证下分发，该许可证授予以修改和未修改的形式使用、研究、更改和共享该软件的所有权利。软件自由对于实现开源软件的社区开发至关重要。 有大量可用的开源软件 - 您会发现其中一些不仅在本单元的背景下有用，而且在您如何学习以及如何进行研究的背景下也很有用。 这里是一个有趣的 CNBC 视频（Youtube平台），讨论了开源软件的兴起 - 它最后提到需要以开放的方式合作应对环境、癌症和阿尔茨海默病等全球挑战。 统计与科学计算 R 与 RStudio Desktop 不言而喻，R 和 RStudio Desktop 是与本课程相关的开源软件的两个最明显的示例。就用于数据分析和统计建模的其他开源语言而言，您可能还对 Python 和 Julia 感兴趣。              Python 虽然 R 往往是对数据整理、数据可视化和统计建模感兴趣的人的首选语言，但从更通用的意义上来说，Python 可以说是”更好”的语言。Python 被机器学习社区广泛使用（仅举例）。 Octave 您可能听说过甚至使用过 MATLAB 进行数值计算。有一个开源的等效项，称为GNU Octave，您可能有兴趣查看一下。 文档创建 到目前为止，您可能主要使用 Microsoft Word 来编写文档。LibreOffice是一个与 Microsoft Office 套件相当的优秀开源软件，提供了大量用于文档编写、处理电子表格和创建演示文稿的应用程序。 如果您有兴趣使用Markdown进行写作（这确实很容易掌握），您可能会对使用HackMD感兴趣。HackMD 是一款 Markdown 编辑器，可让您与他人一起编写协作文档和演示文稿。 构建实验 PsychoPy提供了一个出色的开源解决方案来构建收集人类数据的实验，并且通过配套托管网站Pavlovia提供了一种易于使用的方法来在线运行 PsychoPy 实验。PsychoPy 已经存在很多年了，并且有许多预先构建的实验模板，您可以根据需要进行调整。这里有一个很好的参考资料描述了 PsychoPy 环境。              Linux Linux 操作系统是使用最广泛的开源软件之一。您可以选择使用Linux来运行计算机，而不是在 Windows 或 Mac OS 上运行计算机。Linux 运行着世界上大多数互联网服务器，并且在学术环境中变得越来越流行。Linux 指的是一堆开源的类 Unix 操作系统。它由Linus Torvalds于 1991 年开发并发布。 一些最流行的 Linux 发行版是Ubuntu（我目前使用的1）、Fedora和Debian。如果您真的想进入计算方面的研究，那么探索 Linux 的世界就很重要。 更多开源软件 如果您对开源软件的其他示例感兴趣，您可能有兴趣查看Tech Radar 网站上的开源替代品列表。 开始使用 R 和 RStudio Desktop 在下面的内容中，将向介绍R语言和RStudio Desktop，这是一种集成开发环境 (IDE)，您将使用它来编写可重复的代码，涉及数据的整理、可视化、摘要和统计建模。R 和 RStudio Desktop 都是开源软件的示例。开源是进行可重复研究的关键，因为开源软件是免费的并向所有人开放。 在下面的内容中，将介绍 R 并讨论在我们的研究分析工作流程中采用此类工具的重要性。 为什么是R？ R语言允许您进行可重复的研究和构建可重复的工作流程。R的统计包反映了统计学和数据科学领域的最新进展。 R中有很多令人惊叹的数据可视化包。 在许多机构中，下一代学者（硕士和博士学生、博士后等）正在他们的培训中学习R语言技能。 R编程技能在工业界、ONS、NHS、公务员2等领域非常受欢迎。 开源软件 开源软件是免费的，由社区（包括您在内！）建立、维护、修改和改进的——查看其许可证以获取完整详情。 您可以自由使用开源软件，因此不再被需要付费的专有软件（例如SPSS）所束缚。 开源软件得到了大型公司的支持——例如，Linux得到了包括谷歌、微软、英特尔、三星等在内的Linux基金会的支持。在过去几十年中，开源在公共和私营部门广泛使用，并对计算产生了巨大影响。 选择合适的软件做合适的工作 如果您希望您的研究开放且可重复，不应使用专有（封闭）软件，如Excel、SPSS、GraphPad、MATLAB等。更好的选择是使用开源软件，如R、Octave、Python、Julia等。 您需要使整个分析流程可重复——从数据导入、数据整理、可视化、统计建模到报告生成。编写代码！ 当您想与其他地方或另一个实验室（拥有不同基础设施）的同事共享研究流程分析时会发生什么？ “在我的机器上有效！”——但如果在您的合作者的机器上（或您的新机器上）不起作用，那就不好了。 开源与闭源软件 开源软件使您的数据处理、整理、可视化建模等过程具有透明性和可重复性。 它允许其他人发现并纠正错误… 同时，闭源软件可能有一些风险： R vs. SPSS 下面这段话风趣地对比了R与SPSS “SPSS就像一辆公交车——对标准事物来说使用起来很容易，但如果你想做一些没有预先编程的事情，就会非常沮丧。 R就像一辆带着自行车在后面，皮划艇在顶上，好的步行和跑鞋在副驾驶座位上，以及山地攀登和洞穴探险装备在后备箱的四轮驱动越野车。 如果你愿意花时间学习如何使用这些设备，R可以带你去任何你想去的地方，但这需要的时间会比学习SPSS的公交站点要长。” Greg Snow, 2010, stackoverflow.com 关于R的书籍 《R for Data Science》 这本书还有第二版，但是目前官方尚未翻译为中文。值得注意的是，该书的中文翻译版由国内出版商发行，并未开源获取。 《Advanced R》 Hadley Wickham “彻底变革了R语言的人” RStudio的首席科学家，关键R包的（主要）作者，包括ggplot2、tidyr、dplyr——这些都是tidyverse的组成部分。 R语言被许多机构广泛使用 可以查看这个例子，这个例子描述了BBC的数据新闻团队是如何使用R语言进行绘图的。大家亦可作为参考。 2.0.1 做好准备！上手编写代码！ 做好准备，我们即将开始编写代码。不用担心，学习R是一趟旅途，总会有起伏，但是我们会从不同的角度帮助你思考问题从而将一个个大问题分解为更小的问题，使你可以独立解决他们。如果你之前从未接触过编程语言或者撰写过任何代码，也无需担心。本课程所讲述的所有内容均提前假设参加者没有任何的编码经验。 同样的，在编写代码时，我们会经历不同的困难，并且很可能一直在下面两种状态中转换，这是正常的请不用担心，随着练习的增多，你会逐渐趋于熟练并迎接新的挑战。 这里是 RStudio（和其他组织）的企业家兼创始人 JJ Allaire 的精彩演讲视频。该视频来自 rstudio::conf 2020，其中 JJ 讲述了他从成为一名政治科学家的历程、他如何参与 R 以及开源在可复制数据科学背景下的重要性。如果感兴趣，您可能想观看 RStudio 网站上的其他一些视频。 入门 接下来将展示如何安装 R（语言）和 RStudio Desktop（用于使用该语言的 IDE）。您可以从此处下载适用于各种平台的 R，包括 Mac OS、Windows 和 Ubuntu。要下载免费的 RStudio Desktop，只需转到此处。 如果您遇到了网络访问问题，也可以通过这个飞书链接下载我们预下载的Windows版本的R和MacOs版本的R，以及Windows版本的RStudio Desktop和MacOs版本的RStudio Desktop。 如果您使用的是 Chromebook，或拥有平板电脑，或者在计算机上安装 R 和 RStudio Desktop 时遇到困难，则可以使用RStudio Cloud在浏览器中运行 RStudio 环境。您需要注册，有一个免费计划可供您每月使用 RStudio Cloud 15 小时。不过我们强烈建议您在本地安装R与RStudio Desktop。 好的编码风格 在下面的视频中，将讨论良好的编码风格。当您编写分析脚本时，确保您的代码能够被其他人和未来的您理解是很重要的。如果您尽早养成良好的编码风格的习惯，从长远来看，这将使事情变得更加容易 - 并且您会发现协作工作更容易，因为其他人会发现与您合作更容易。 您可以在此处查看有用的 Tidyverse 风格指南。 如果您想让您的代码和数据开放（您确实应该这样做，除非有充分的理由不这样做），那么正确许可它以允许其他人（重新）使用和重新混合它非常重要。对于我自己的工作，我倾向于使用尽可能宽松的许可证。我的“首选”许可证是MIT许可证和知识共享许可证 CC-BY 4.0。 如果您需要帮助为自己的作品选择正确的许可证，您可以使用这个方便的指南。 开始你的第一个脚本 您现在将运行您的第一个 R 脚本。我们将使用多年来超过 80,000 次 UFO 目击事件的数据库创建美国 UFO 目击事件的三种可视化效果。在运行代码之前，您需要在计算机上安装两个软件包 - 它们是tidyverse和patchwork。 安装软件包后，将以下代码键入新的 R 脚本中。脚本中的数据集可以在这个链接中下载。按照视频中向展示的相同方式运行代码。您的可视化看起来和我的一样嘛？ library(tidyverse) # 加载 tidyverse 包 library(patchwork) # 使用这个包将图拼接在一起 # 读取数据，在这里，我将数据存储在了Datasets文件夹下。 ufo_sightings &lt;- read_csv(&quot;Datasets/ufo_sightings.csv&quot;) # 作图：美国前 10 个州的目击事件数量 plot1 &lt;- ufo_sightings %&gt;% filter(!is.na(state)) %&gt;% mutate(state = str_to_upper(state)) %&gt;% group_by(state) %&gt;% tally() %&gt;% top_n(10) %&gt;% ggplot(aes(x = reorder(state, n), y = n, fill = state)) + geom_col() + coord_flip() + guides(fill = &quot;none&quot;) + labs(title = &quot;Top 10 States for UFO Sightings&quot;, x = NULL, y = NULL) + ylim(0, 11000) + theme_minimal() + theme(text = element_text(size = 15)) # 计算出纬度和经度范围内的州（即排除阿拉斯加） tidied_ufo &lt;- ufo_sightings %&gt;% filter(country == &quot;us&quot;) %&gt;% filter(latitude &gt; 24 &amp; latitude &lt; 50) # 在美国地图上绘制所有目击事件 plot2 &lt;- tidied_ufo %&gt;% ggplot(aes(x = longitude, y = latitude)) + geom_point(size = .5, alpha = .25) + theme_void() + coord_cartesian() + labs(title = &quot;Sites of UFO Sightings in the US&quot;) + theme(text = element_text(size = 15)) # 作图：加利福尼亚州发现的十大 UFO 形状图 plot3 &lt;- tidied_ufo %&gt;% filter(state == &quot;ca&quot;) %&gt;% filter(ufo_shape != &quot;other&quot;) %&gt;% filter(ufo_shape != &quot;unknown&quot;) %&gt;% group_by(ufo_shape) %&gt;% tally() %&gt;% top_n(10) %&gt;% mutate(ufo_shape = str_to_title(ufo_shape)) %&gt;% ggplot(aes(x = reorder(ufo_shape, n), y = n, fill = ufo_shape)) + geom_col() + coord_flip() + guides(fill = &quot;none&quot;) + labs(title = &quot;Top 10 UFO Shapes spotted in California&quot;, x = NULL, y = NULL) + theme_minimal() + theme(text = element_text(size = 15)) # 将图拼接在一起 my_plot &lt;- (plot1 + plot3) / (plot2) ggsave(&quot;ufo_plot.jpg&quot;, plot = my_plot, width = 12, height = 10) 帮助我们改进本节课程 此处应插入文本和反馈二维码或问卷链接 笔者注：这里指原文的作者Prof. Andrew Stewart。可以看得出他确实是一名开源软件的爱好者，当在课上谈起Linux时，他讲了许多并十分推荐我们尝试Linux，并且眼睛似乎都在放光↩︎ 笔者注：这里指英国公务员↩︎ "],["数据整理和总结.html", "Chapter 3 数据整理和总结 数据整理 概括你的数据 帮助我们改进本节课程", " Chapter 3 数据整理和总结 在本次研讨会中，我将向您介绍许多关键包，Tidyverse这些包包含大量用于处理整齐格式数据的函数。通过使我们的数据整理可重现（即，通过用 R 进行编码），我们可以在添加新数据时轻松地重新运行分析流程。数据整理阶段的可重复性是分析过程的关键部分，但在需要确保其可重复性方面经常被忽视。本次研讨会分为两个部分。第一个重点是数据整理，第二部分是数据总结。 数据整理 概述 我们将首先了解Tidyverse中的一些关键工具，这些工具使我们能够整理和整理数据，使其达到我们可视化和建模所需的格式。Tidyverse 是一个相互“很好地配合”的软件包的集合。它们基于一个共同的理念，即数据以矩形格式（即行和列）表示。这些矩形结构在 Tidyverse 中被称为tibbles。如果您有兴趣，可以在此处tibbles阅读R4DS 书籍中的更多信息。 请观看以下视频，我将在其中引导您完成此工作表。然后我希望您通过在自己的计算机上编写（并运行）脚本来完成内容。 加载Tidyverse 让我们首先看一下数据整理。我们将从 Tidyverse 附带的数据集开始。该数据集mpg包含 1999 年至 2008 年美国 38 种流行车型的燃油经济性数据。 首先，我们需要tidyverse使用以下内容加载库： library(tidyverse) 如果您在没有先在计算机上安装 Tidyverse 的情况下运行此行，您将遇到错误。R 包只需要安装一次，因此如果您想第一次将其加载到库中，则需要使用install.packages(*\"packagename\"*)。 对于tidyverse我们需要安装它： install.packages(&quot;tidyverse&quot;) 安装tidyverse后，您可以使用library()功能将其加载到您的库中。您只需在计算机上安装一次软件包（除非您已更新 R 或者您想要安装特定软件包的最新版本）。当您编写 R 脚本时，您永远不想在脚本主体中拥有该函数install.packages()，就好像其他人运行您的脚本一样，这会更新他们计算机上的包（他们可能不想要）。 数据集mpg mpg数据集作为Tidyverse包帮助文件的一部分被加载，可以使用help(mpg)或者?mpg来查看，我们可以看到下面的说明： Description This dataset contains a subset of the fuel economy data that the EPA makes available on http://fueleconomy.gov. It contains only models which had a new release every year between 1999 and 2008 - this was used as a proxy for the popularity of the car. A data frame with 234 rows and 11 variables. manufacturer - manufacturer model - model name displ - engine displacement, in litres year - year of manufacture cyl - number of cylinders trans -type of transmission drv-f = front-wheel drive, r = rear wheel drive, 4 = 4wd cty - city miles per gallon hwy - highway miles per gallon fl - fuel type class - “type” of car 使用head()和str() 我们可以通过多种方式探索 Tidyverse 加载的数据集mpg。如果我们想查看数据集的前 6 行，我们可以使用该head()函数。 head(mpg) ## # A tibble: 6 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compa… ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compa… ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compa… ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compa… ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compa… ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compa… 我们看到它是一个tibble或者说一个矩形数据框，由行和列组成。这是每个观察对应一行的格式tidy。我们将在 R 中运行的大多数分析都涉及tidy数据。在 Tidyverse 中，tibble是表示数据的标准方式。您将花费大量时间整理和整理数据以将其转换为这种格式！通过使用您编写的脚本在 R 中执行此操作，您可以使这个关键阶段可重现。您可以在更新的或不同的数据集上再次运行脚本,因此可能会节省您大量时间！ 我们还可以使用 询问有关数据集结构的信息str()。这将告诉我们有关列的信息、每个变量的类型、行数等。 str(mpg) ## tibble [234 × 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ trans : chr [1:234] &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ class : chr [1:234] &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ... 用select()选择列 如果我们愿意，我们可以使用该select()函数选择其中一列。下面我们只选择标题为manufacturer的列。 mpg %&gt;% select(manufacturer) ## # A tibble: 234 × 1 ## manufacturer ## &lt;chr&gt; ## 1 audi ## 2 audi ## 3 audi ## 4 audi ## 5 audi ## 6 audi ## 7 audi ## 8 audi ## 9 audi ## 10 audi ## # ℹ 224 more rows 与select()功能相关的是rename()和您想的一样，它重命名一列。 我们还可以使用该distinct()函数查看数据集中的不同汽车制造商。这为我们提供了不同的制造商名称。如果您想检查数据集是否有（例如）参与者 ID 的重复项，此功能会非常方便。 mpg %&gt;% distinct(manufacturer) ## # A tibble: 15 × 1 ## manufacturer ## &lt;chr&gt; ## 1 audi ## 2 chevrolet ## 3 dodge ## 4 ford ## 5 honda ## 6 hyundai ## 7 jeep ## 8 land rover ## 9 lincoln ## 10 mercury ## 11 nissan ## 12 pontiac ## 13 subaru ## 14 toyota ## 15 volkswagen 用filter()选择行 有时我们可能只想选择数据集中的行子集。我们可以使用该filter()函数来做到这一点。例如，在这里我们过滤数据集以仅包含“honda”制造的汽车。 mpg %&gt;% filter(manufacturer == &quot;honda&quot;) ## # A tibble: 9 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 honda civic 1.6 1999 4 manual(m5) f 28 33 r subco… ## 2 honda civic 1.6 1999 4 auto(l4) f 24 32 r subco… ## 3 honda civic 1.6 1999 4 manual(m5) f 25 32 r subco… ## 4 honda civic 1.6 1999 4 manual(m5) f 23 29 p subco… ## 5 honda civic 1.6 1999 4 auto(l4) f 24 32 r subco… ## 6 honda civic 1.8 2008 4 manual(m5) f 26 34 r subco… ## 7 honda civic 1.8 2008 4 auto(l5) f 25 36 r subco… ## 8 honda civic 1.8 2008 4 auto(l5) f 24 36 c subco… ## 9 honda civic 2 2008 4 manual(m6) f 21 29 p subco… 请注意，我们使用的运算符==表示“等于”。这是一个逻辑运算符,其他逻辑运算符包括小于&lt;、大于&gt;、小于等于&lt;=、大于等于&gt;=和不等于!=。 下面我们过滤制造商为“honda”且制造年份为“1999”的情况。 mpg %&gt;% filter(manufacturer == &quot;honda&quot; &amp; year == &quot;1999&quot;) ## # A tibble: 5 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 honda civic 1.6 1999 4 manual(m5) f 28 33 r subco… ## 2 honda civic 1.6 1999 4 auto(l4) f 24 32 r subco… ## 3 honda civic 1.6 1999 4 manual(m5) f 25 32 r subco… ## 4 honda civic 1.6 1999 4 manual(m5) f 23 29 p subco… ## 5 honda civic 1.6 1999 4 auto(l4) f 24 32 r subco… 组合功能 我们可以结合使用filter()和select()来过滤制造商为“honda”、制造年份为“1999”的情况，并且我们只想显示这两列以及告诉我们燃油经济性的列(cty和hwy)。 mpg %&gt;% filter(manufacturer == &quot;honda&quot; &amp; year == &quot;1999&quot;) %&gt;% select(manufacturer, year, cty, hwy) ## # A tibble: 5 × 4 ## manufacturer year cty hwy ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 honda 1999 28 33 ## 2 honda 1999 24 32 ## 3 honda 1999 25 32 ## 4 honda 1999 23 29 ## 5 honda 1999 24 32 通过组合几个函数，您可以想象我们可以非常简单地构建一些相当复杂的数据整理规则。 管道操作符%&gt;% 请注意，在上面的这些示例中，我们使用了%&gt;%运算符 - 这称为管道，允许我们将信息从管道的一侧传递到另一侧。您可以将其读作“然后”。Tidyverse 中的所有函数（例如select()和filter()）都称为动词（verbs），它们本身描述了自己的作用。管道是 Tidyverse 中最常用的运算符之一，它允许我们将不同的代码行链接在一起 - 每行的输出作为输入传递到下一行。在此示例中，数据集mpg被传递到distinct()函数，我们在其中请求不同（即唯一）制造商的列表。这个输出本身就是一个向量。向量是一种基本数据结构，包含相同类型的元素 - 例如，一堆数字。我们可以在管道链中添加另一行来告诉我们这个向量中有多少个元素。我们可以将其大声朗读为“获取数据集 mpg，然后计算出不同的制造商名称，然后对它们进行计数”。 mpg %&gt;% distinct(manufacturer) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 15 整理数据集 整理变量名称 目前，汽车制造商名称均为小写。如果它们采用标题大小写（即每个单词的第一个字母大写），看起来会好得多。我们可以使用该mutate()函数创建一个新列 - 这次，新列的名称,即是我们要使用该str_to_title()函数修改旧列的名称。这将覆盖该列并将manufacturer其替换为首字母大写的新版本。 mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 Audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 Audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 Audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 Audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 Audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 Audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 Audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 Audi a4 quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 Audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… ## # ℹ 224 more rows 列model也是小写的。我们也来让他首字母大写。我们可以使用该mutate()函数同时处理多个列，如下所示： mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer), model = str_to_title(model)) ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Audi A4 1.8 1999 4 auto… f 18 29 p comp… ## 2 Audi A4 1.8 1999 4 manu… f 21 29 p comp… ## 3 Audi A4 2 2008 4 manu… f 20 31 p comp… ## 4 Audi A4 2 2008 4 auto… f 21 30 p comp… ## 5 Audi A4 2.8 1999 6 auto… f 16 26 p comp… ## 6 Audi A4 2.8 1999 6 manu… f 18 26 p comp… ## 7 Audi A4 3.1 2008 6 auto… f 18 27 p comp… ## 8 Audi A4 Quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 Audi A4 Quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 Audi A4 Quattro 2 2008 4 manu… 4 20 28 p comp… ## # ℹ 224 more rows 数据集里有很多列，所以我们只选择manufacturer, model, year, transmission 和 hwy列： mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer), model = str_to_title(model)) %&gt;% select(manufacturer, model, year, trans, hwy) ## # A tibble: 234 × 5 ## manufacturer model year trans hwy ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Audi A4 1999 auto(l5) 29 ## 2 Audi A4 1999 manual(m5) 29 ## 3 Audi A4 2008 manual(m6) 31 ## 4 Audi A4 2008 auto(av) 30 ## 5 Audi A4 1999 auto(l5) 26 ## 6 Audi A4 1999 manual(m5) 26 ## 7 Audi A4 2008 auto(av) 27 ## 8 Audi A4 Quattro 1999 manual(m5) 26 ## 9 Audi A4 Quattro 1999 auto(l5) 25 ## 10 Audi A4 Quattro 2008 manual(m6) 28 ## # ℹ 224 more rows 重新编码变量 在现实世界中，数据集（Data Frames）并不总是以整齐的格式到达我们的计算机。通常，您需要先进行一些数据整理，然后才能对它们进行任何有用的操作。我们将看一个示例，说明如何从杂乱的数据变为整洁的数据。使用的数据集可以在这里下载 my_messy_data &lt;- read_csv(&quot;Dataset/my_data.csv&quot;) my_messy_data 我们对 24 名参与者和 4 个条件进行了反应时间实验，他们在我们的数据文件中编号为 1-4。 head(my_messy_data) ## # A tibble: 6 × 3 ## participant condition rt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 879 ## 2 1 2 1027 ## 3 1 3 1108 ## 4 1 4 765 ## 5 2 1 1042 ## 6 2 2 1050 这是一种重复测量设计，其中一个因素（启动类型,即Prime Type）具有两个水平（A 与 B），第二个因素（目标类型,即Target Type）具有两个水平（A 与 B）。我们想要重新编码我们的数据框，以便它更好地匹配我们的实验设计。首先我们需要像这样重新编码 4 个情况（Condition）： 重新编码Condition列如下： - Condition 1 = Prime A, Target A - Condition 2 = Prime A, Target B - Condition 3 = Prime B, Target A - Condition 4 = Prime B, Target B my_messy_data %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;PrimeA_TargetA&quot;, &quot;2&quot; = &quot;PrimeA_TargetB&quot;, &quot;3&quot; = &quot;PrimeB_TargetA&quot;, &quot;4&quot; = &quot;PrimeB_TargetB&quot;)) %&gt;% head() ## # A tibble: 6 × 3 ## participant condition rt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 PrimeA_TargetA 879 ## 2 1 PrimeA_TargetB 1027 ## 3 1 PrimeB_TargetA 1108 ## 4 1 PrimeB_TargetB 765 ## 5 2 PrimeA_TargetA 1042 ## 6 2 PrimeA_TargetB 1050 我们现在需要将我们的条件列分成两个 - 一个用于我们的第一个因素（Prime），另一个用于我们的第二个因素（Target）。separate() 函数正是做这个的 — 当与管道化的tibble一起使用时，它需要知道我们想要分离哪一列，通过分离原始列来创建什么新列，以及我们想要基于什么来进行分离。在下面的示例中，我们告诉 separate() 我们想要将标记为 condition 的列分成两个新列，称为 Prime 和 Target，并且我们想要在要被分离的列中出现 _ 的任何地方进行这种分离。 my_messy_data %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;PrimeA_TargetA&quot;, &quot;2&quot; = &quot;PrimeA_TargetB&quot;, &quot;3&quot; = &quot;PrimeB_TargetA&quot;, &quot;4&quot; = &quot;PrimeB_TargetB&quot;)) %&gt;% separate(col = &quot;condition&quot;, into = c(&quot;Prime&quot;, &quot;Target&quot;), sep = &quot;_&quot;) ## # A tibble: 96 × 4 ## participant Prime Target rt ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 PrimeA TargetA 879 ## 2 1 PrimeA TargetB 1027 ## 3 1 PrimeB TargetA 1108 ## 4 1 PrimeB TargetB 765 ## 5 2 PrimeA TargetA 1042 ## 6 2 PrimeA TargetB 1050 ## 7 2 PrimeB TargetA 942 ## 8 2 PrimeB TargetB 945 ## 9 3 PrimeA TargetA 943 ## 10 3 PrimeA TargetB 910 ## # ℹ 86 more rows my_messy_data %&gt;% mutate(condition = recode(condition, &quot;1&quot; = &quot;PrimeA_TargetA&quot;, &quot;2&quot; = &quot;PrimeA_TargetB&quot;, &quot;3&quot; = &quot;PrimeB_TargetA&quot;, &quot;4&quot; = &quot;PrimeB_TargetB&quot;)) %&gt;% separate(col = &quot;condition&quot;, into = c(&quot;Prime&quot;, &quot;Target&quot;), sep = &quot;_&quot;) %&gt;% mutate(Prime = factor(Prime), Target = factor(Target)) ## # A tibble: 96 × 4 ## participant Prime Target rt ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 PrimeA TargetA 879 ## 2 1 PrimeA TargetB 1027 ## 3 1 PrimeB TargetA 1108 ## 4 1 PrimeB TargetB 765 ## 5 2 PrimeA TargetA 1042 ## 6 2 PrimeA TargetB 1050 ## 7 2 PrimeB TargetA 942 ## 8 2 PrimeB TargetB 945 ## 9 3 PrimeA TargetA 943 ## 10 3 PrimeA TargetB 910 ## # ℹ 86 more rows pivot 函数 我们将在 R 中进行的大多数分析都要求数据处于整洁或长格式。在这样的数据集中，一行对应一个观测值。在现实世界中，数据很少处于适合分析的正确格式。在 R 中，pivot_wider() 和 pivot_longer() 函数被设计用来重塑我们的数据文件。首先，让我们加载一个处于宽格式的数据文件（即，每行有多个观测值）。它来自一个我们进行了四种条件（标记为 Condition1、Condition2、Condition3 和 Condition4）的实验。除了有四种条件的每一列之外，我们还有一列对应于参与者 ID。数据集中的每个单元格对应一个反应时间（以毫秒为单位）。数据可以在这里下载 my_wide_data &lt;- read_csv(&quot;Dataset/my_wide_data.csv&quot;) ## Rows: 32 Columns: 5 ## ── Column specification ────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): ID, Condition1, Condition2, Condition3, Condition4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. pivot_longer() 函数 head(my_wide_data) ## # A tibble: 6 × 5 ## ID Condition1 Condition2 Condition3 Condition4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 487 492 499 488 ## 2 2 502 494 517 508 ## 3 3 510 483 488 509 ## 4 4 476 488 513 521 ## 5 5 504 478 513 504 ## 6 6 505 486 503 495 所以，我们可以看到数据文件是宽格式的。我们想要将其重塑为长格式。我们可以使用 pivot_longer() 函数来做到这一点。 最低限度地，我们需要指定我们想要重塑的数据框架，我们想要转换为长格式的列，我们正在创建的新列的名称，以及将保存我们重塑数据框架（data frame）的值的列的名称。我们将输出映射到一个我称之为 my_longer_data 的变量。 my_longer_data &lt;- my_wide_data %&gt;% pivot_longer(cols = c(Condition1, Condition2, Condition3, Condition4), names_to = &quot;Condition&quot;, values_to = &quot;RT&quot;) 现在我们来看看重塑后的数据框架的样子 head(my_longer_data) ## # A tibble: 6 × 3 ## ID Condition RT ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Condition1 487 ## 2 1 Condition2 492 ## 3 1 Condition3 499 ## 4 1 Condition4 488 ## 5 2 Condition1 502 ## 6 2 Condition2 494 所以你可以看到我们的数据现在是长格式（或tidy格式），每行一个观察。注意我们的Condition列没有被编码为因子。由于我们的数据集需要反映我们实验的结构，因此让我们将该列转换为因子 - 注意在以下代码中我们现在正在“保存”更改，因为我们没有将输出映射到变量名上。 my_longer_data %&gt;% mutate(Condition = factor(Condition)) %&gt;% head() ## # A tibble: 6 × 3 ## ID Condition RT ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Condition1 487 ## 2 1 Condition2 492 ## 3 1 Condition3 499 ## 4 1 Condition4 488 ## 5 2 Condition1 502 ## 6 2 Condition2 494 pivot_wider()函数 我们可以使用 pivot_wider() 函数来重塑长数据框，使其从长格式转换为宽格式。它的工作方式类似于 pivot_longer()。让我们拿我们新的、长的、数据框并将其转回宽格式。使用 pivot_wider() 时，我们最低需要指定我们想要重塑的数据框，并指定一对参数（names_from 和 values_from），这些参数描述了从哪个列获取输出列的名称，以及从哪个列获取单元格值。 my_wider_data &lt;- my_longer_data %&gt;% pivot_wider(names_from = &quot;Condition&quot;, values_from = &quot;RT&quot;) 我们可以检查我们的数据集是否已经回到宽格式。 head(my_wider_data) ## # A tibble: 6 × 5 ## ID Condition1 Condition2 Condition3 Condition4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 487 492 499 488 ## 2 2 502 494 517 508 ## 3 3 510 483 488 509 ## 4 4 476 488 513 521 ## 5 5 504 478 513 504 ## 6 6 505 486 503 495 合并两个数据集 有时你可能需要合并两个数据集。例如，你可能有一个包含阅读时间数据的数据集（就像上面那个）和另一个包含第一个数据集中参与者的个体差异测量的数据集。我们如何才能合并这两个数据集，以便最终得到一个既包括阅读时间数据又包括个体差异测量（我们可能稍后想要协变掉）的数据集？幸运的是，dplyr 包包含了许多连接函数，允许你将不同的tibbles连接在一起。首先，让我们加载包含个体差异测量的数据。该数据集可以在这里下载 individual_diffs &lt;- read_csv(&quot;Dataset/individual_diffs.csv&quot;) 让我们看看个体差异数据的前几行。这个数据集包含了我们参与者的ID号以及智商（iq列）和工作记忆（wm列）的测量值。 head(individual_diffs) ## # A tibble: 6 × 3 ## ID iq wm ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 100 9 ## 2 2 108 8 ## 3 3 116 9 ## 4 4 95 9 ## 5 5 83 11 ## 6 6 73 10 完全合并 我们可以使用其中一个连接函数来组合。有多种选择，包括 full_join()，它包括了我们想要连接的第一个或第二个 tibble 中的所有行。其他选项包括 inner_join()，它包括了第一个和第二个 tibble 中的所有行，以及 left_join() 和 right_join()。 combined_data &lt;- full_join(my_longer_data, individual_diffs, by = &quot;ID&quot;) 我们现在看到我们的数据集按照我们的预期组合在一起了。 combined_data ## # A tibble: 128 × 5 ## ID Condition RT iq wm ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Condition1 487 100 9 ## 2 1 Condition2 492 100 9 ## 3 1 Condition3 499 100 9 ## 4 1 Condition4 488 100 9 ## 5 2 Condition1 502 108 8 ## 6 2 Condition2 494 108 8 ## 7 2 Condition3 517 108 8 ## 8 2 Condition4 508 108 8 ## 9 3 Condition1 510 116 9 ## 10 3 Condition2 483 116 9 ## # ℹ 118 more rows 左合并 当然，你可能会想，我们可以简单地使用Excel对我们想要的列进行剪切和粘贴，从一个数据集转移到另一个数据集。但是，如果我们的个体差异文件包含了10,000个参与者的ID（以随机顺序排列），而我们只对在两个数据集匹配的情况下进行合并感兴趣呢？数据可以在这里下载 large_ind_diffs &lt;- read_csv(&quot;Dataset/large_ind_diffs.csv&quot;) head(large_ind_diffs) ## # A tibble: 6 × 3 ## ID iq wm ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6057 93 7 ## 2 2723 86 7 ## 3 1088 97 9 ## 4 8687 87 8 ## 5 4223 77 11 ## 6 369 95 9 我们实际上可以使用另一个连接函数（left_join()）来组合这两个数据集，但只在第一个数据集（函数调用中的my_longer_data）的ID与另一个数据集匹配的情况下进行组合。 left_join(my_longer_data, large_ind_diffs, by = &quot;ID&quot;) ## # A tibble: 128 × 5 ## ID Condition RT iq wm ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Condition1 487 100 9 ## 2 1 Condition2 492 100 9 ## 3 1 Condition3 499 100 9 ## 4 1 Condition4 488 100 9 ## 5 2 Condition1 502 108 8 ## 6 2 Condition2 494 108 8 ## 7 2 Condition3 517 108 8 ## 8 2 Condition4 508 108 8 ## 9 3 Condition1 510 116 9 ## 10 3 Condition2 483 116 9 ## # ℹ 118 more rows 作业 尝试通过使用 RStudio Desktop 编写你的第一个脚本来重现上面的内容。 概括你的数据 概述 一旦数据集被整理好，我们通常想要做的第一件事之一就是生成摘要统计数据。在这个研讨会中，我们将使用内置于 tidyverse 中的 mpg 数据集。这个数据集包含了许多不同制造商生产的汽车的信息（如发动机大小、燃油经济性等）。我们如何生成（例如）按汽车制造商分组的某个变量的平均值和标准偏差？请观看以下视频，我将引导你完成这个工作表。然后，我希望你通过在自己的机器上编写（并运行）脚本来完成内容。 在继续之前，请记得为这节课设置一个 .Rproj 文件。在你的脚本中，你首先需要加载 tidyverse。 library(tidyverse) 使用 group_by() 和 summarise() 我们将使用group_by()函数对数据集进行分组，然后使用summarise()函数计算hwy变量的均值和标准差。summarise()函数可以使用许多不同的函数来提供摘要统计信息。要了解更多不同选项，请在控制台窗口中键入?summarise。常用的函数包括mean()、median()和sd()。 mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) ## # A tibble: 15 × 4 ## manufacturer mean_hwy sd_hwy number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 audi 26.4 2.18 18 ## 2 chevrolet 21.9 5.11 19 ## 3 dodge 17.9 3.57 37 ## 4 ford 19.4 3.33 25 ## 5 honda 32.6 2.55 9 ## 6 hyundai 26.9 2.18 14 ## 7 jeep 17.6 3.25 8 ## 8 land rover 16.5 1.73 4 ## 9 lincoln 17 1 3 ## 10 mercury 18 1.15 4 ## 11 nissan 24.6 5.09 13 ## 12 pontiac 26.4 1.14 5 ## 13 subaru 25.6 1.16 14 ## 14 toyota 24.9 6.17 34 ## 15 volkswagen 29.2 5.32 27 请注意，此输出当前按第一列 manufacturer 的字母顺序排列。如果我们想按高速公路平均燃油经济性从高到低的顺序排列呢？我们可以使用 arrange() 函数。 使用arrange()重新排序输出 mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) %&gt;% arrange(mean_hwy) ## # A tibble: 15 × 4 ## manufacturer mean_hwy sd_hwy number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 land rover 16.5 1.73 4 ## 2 lincoln 17 1 3 ## 3 jeep 17.6 3.25 8 ## 4 dodge 17.9 3.57 37 ## 5 mercury 18 1.15 4 ## 6 ford 19.4 3.33 25 ## 7 chevrolet 21.9 5.11 19 ## 8 nissan 24.6 5.09 13 ## 9 toyota 24.9 6.17 34 ## 10 subaru 25.6 1.16 14 ## 11 pontiac 26.4 1.14 5 ## 12 audi 26.4 2.18 18 ## 13 hyundai 26.9 2.18 14 ## 14 volkswagen 29.2 5.32 27 ## 15 honda 32.6 2.55 9 emmmmmmmm，这不是我们想要的,这是从最低到最高的顺序，这是R的默认顺序。我们可以通过在参数前加上-符号来改变这一点。 mpg %&gt;% group_by(manufacturer) %&gt;% summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy), number = n()) %&gt;% arrange(-mean_hwy) ## # A tibble: 15 × 4 ## manufacturer mean_hwy sd_hwy number ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 honda 32.6 2.55 9 ## 2 volkswagen 29.2 5.32 27 ## 3 hyundai 26.9 2.18 14 ## 4 audi 26.4 2.18 18 ## 5 pontiac 26.4 1.14 5 ## 6 subaru 25.6 1.16 14 ## 7 toyota 24.9 6.17 34 ## 8 nissan 24.6 5.09 13 ## 9 chevrolet 21.9 5.11 19 ## 10 ford 19.4 3.33 25 ## 11 mercury 18 1.15 4 ## 12 dodge 17.9 3.57 37 ## 13 jeep 17.6 3.25 8 ## 14 lincoln 17 1 3 ## 15 land rover 16.5 1.73 4 这看起来更好了。 summarise_at() 的变体 除了使用summarise()之外，您还可以使用相关的函数，例如summarise_at()。这是summarise()函数的作用范围版本，可以应用于多个列。请注意，在使用summarise_at()时，您需要用引号将要进行汇总的列括起来。您还需要提供汇总函数 - 在本例中为mean。最后，如果我们的数据集包含任何缺失值（由NA表示），我们将设置参数na.rm = TRUE。这将确保在应用操作之前移除缺失的数据点。如果我们有缺失的数据，但没有告诉R我们想要做什么，它会抛出一个错误。 mpg %&gt;% group_by(manufacturer) %&gt;% summarise_at(c(&quot;displ&quot;, &quot;cty&quot;, &quot;hwy&quot;), mean, na.rm = TRUE) ## # A tibble: 15 × 4 ## manufacturer displ cty hwy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 2.54 17.6 26.4 ## 2 chevrolet 5.06 15 21.9 ## 3 dodge 4.38 13.1 17.9 ## 4 ford 4.54 14 19.4 ## 5 honda 1.71 24.4 32.6 ## 6 hyundai 2.43 18.6 26.9 ## 7 jeep 4.58 13.5 17.6 ## 8 land rover 4.3 11.5 16.5 ## 9 lincoln 5.4 11.3 17 ## 10 mercury 4.4 13.2 18 ## 11 nissan 3.27 18.1 24.6 ## 12 pontiac 3.96 17 26.4 ## 13 subaru 2.46 19.3 25.6 ## 14 toyota 2.95 18.5 24.9 ## 15 volkswagen 2.26 20.9 29.2 summarise_if() 的变体 假设我们有一个非常庞大的数据集，并且希望总结所有特定类型的列。我们可以使用summarise_if()函数来计算每个汽车制造商的平均值，如下所示： mpg %&gt;% group_by(manufacturer) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## # A tibble: 15 × 6 ## manufacturer displ year cyl cty hwy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi 2.54 2004. 5.22 17.6 26.4 ## 2 chevrolet 5.06 2005. 7.26 15 21.9 ## 3 dodge 4.38 2004. 7.08 13.1 17.9 ## 4 ford 4.54 2003. 7.2 14 19.4 ## 5 honda 1.71 2003 4 24.4 32.6 ## 6 hyundai 2.43 2004. 4.86 18.6 26.9 ## 7 jeep 4.58 2006. 7.25 13.5 17.6 ## 8 land rover 4.3 2004. 8 11.5 16.5 ## 9 lincoln 5.4 2002 8 11.3 17 ## 10 mercury 4.4 2004. 7 13.2 18 ## 11 nissan 3.27 2004. 5.54 18.1 24.6 ## 12 pontiac 3.96 2003. 6.4 17 26.4 ## 13 subaru 2.46 2004. 4 19.3 25.6 ## 14 toyota 2.95 2003. 5.12 18.5 24.9 ## 15 volkswagen 2.26 2003. 4.59 20.9 29.2 在summarise_if()中的第一个参数是应用于每一列的逻辑测试 - 在这种情况下，如果一列是数值（即，一个数字） - 那么测试结果为TRUE，然后应用第二个函数mean。同样，我们告诉R忽略缺失的（NA）数据值，使用na.rm = TRUE参数。 R函数在参数方面有所不同。我经常忘记它们,如果你开始输入一个函数名，你会在你输入的位置上方得到一个小气泡，提醒你需要哪些参数。如果你记不住细节，你可以在控制台中键入help(function_name)或?function_name来获取任何需要帮助的函数的帮助。使用R（或Python或任何其他语言）进行大量的数据分析通常需要大量的搜索。这是正常的。有些东西我永远记不住，总是不得不查一下！ 使用mutate()添加列 我们可以使用mutate()函数像这样添加一个新的列，我称之为mean_hwy。 mpg %&gt;% group_by(manufacturer) %&gt;% mutate(mean_hwy = mean(hwy), sd_hwy = sd(hwy)) ## # A tibble: 234 × 13 ## # Groups: manufacturer [15] ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 audi a4 quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… ## # ℹ 224 more rows ## # ℹ 2 more variables: mean_hwy &lt;dbl&gt;, sd_hwy &lt;dbl&gt; 我们在这个页面上有太多的列要显示，所以我们可以通过稍微不同地使用select()函数来删除一些列。通过在select()中的列名前面加上-符号，我们最终删除了它。 mpg %&gt;% group_by(manufacturer) %&gt;% mutate(mean_hwy = mean(hwy), sd_hwy = sd(hwy)) %&gt;% select(-class, -trans) ## # A tibble: 234 × 11 ## # Groups: manufacturer [15] ## manufacturer model displ year cyl drv cty hwy fl mean_hwy sd_hwy ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi a4 1.8 1999 4 f 18 29 p 26.4 2.18 ## 2 audi a4 1.8 1999 4 f 21 29 p 26.4 2.18 ## 3 audi a4 2 2008 4 f 20 31 p 26.4 2.18 ## 4 audi a4 2 2008 4 f 21 30 p 26.4 2.18 ## 5 audi a4 2.8 1999 6 f 16 26 p 26.4 2.18 ## 6 audi a4 2.8 1999 6 f 18 26 p 26.4 2.18 ## 7 audi a4 3.1 2008 6 f 18 27 p 26.4 2.18 ## 8 audi a4 qu… 1.8 1999 4 4 18 26 p 26.4 2.18 ## 9 audi a4 qu… 1.8 1999 4 4 16 25 p 26.4 2.18 ## 10 audi a4 qu… 2 2008 4 4 20 28 p 26.4 2.18 ## # ℹ 224 more rows 请注意，这不会永久更改mpg数据集 - 除非我们将此代码的输出映射到新变量，否则不会保存更改。 下面我通过使用赋值运算符&lt;-将其映射到我称为mpg_with_mean新变量来执行此操作。 请注意，我们在最后删除了分组，因为我们不希望分组规则保留在新的数据框中。 mpg_with_mean &lt;- mpg %&gt;% group_by(manufacturer) %&gt;% mutate(mean_hwy = mean(hwy), sd_hyw = sd(hwy)) %&gt;% ungroup() %&gt;% select(-class, -trans) 然后我们可以使用head()和str()检查这个新变量。 head(mpg_with_mean) ## # A tibble: 6 × 11 ## manufacturer model displ year cyl drv cty hwy fl mean_hwy sd_hyw ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 audi a4 1.8 1999 4 f 18 29 p 26.4 2.18 ## 2 audi a4 1.8 1999 4 f 21 29 p 26.4 2.18 ## 3 audi a4 2 2008 4 f 20 31 p 26.4 2.18 ## 4 audi a4 2 2008 4 f 21 30 p 26.4 2.18 ## 5 audi a4 2.8 1999 6 f 16 26 p 26.4 2.18 ## 6 audi a4 2.8 1999 6 f 18 26 p 26.4 2.18 str(mpg_with_mean) ## tibble [234 × 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ mean_hwy : num [1:234] 26.4 26.4 26.4 26.4 26.4 ... ## $ sd_hyw : num [1:234] 2.18 2.18 2.18 2.18 2.18 ... 作业 Tidyverse还有许多其他内置数据集。 另一个是starwars数据集。 您可以通过输入starwars或输入view(starwars)来查看它。 第二个选项将在新窗口中打开数据集。 尝试一下吧。 计算出星球大战宇宙中人类的平均身高。 可能存在一些缺失数据（由NA指示）。 您可以在summarise()函数中使用na.rm = TRUE参数来在生成汇总统计信息时忽略这些值。 过滤掉NA值的另一种方法是在管道中使用filter()函数。 函数is.na()返回逻辑值 TRUE 或 FALSE。 操作符! 表示 NOT，因此当高度值存在时，表达式!is.na(height)将返回 TRUE，如果不存在，则返回 FALSE。 通过将其与filter()相结合，我们得到了行filter(!is.na(height)) ，它将仅过滤我们有高度数据的情况（即!is.na(height)为 TRUE）。 所以你的代码可能如下所示： starwars %&gt;% filter(!is.na(height)) %&gt;% filter(species == &quot;Human&quot;) %&gt;% summarise(mean_height = mean(height)) ## # A tibble: 1 × 1 ## mean_height ## &lt;dbl&gt; ## 1 177. 将summarise()行中的单词mean替换为median 。 你还可以用什么其他东西来代替它？ 提示：在控制台中输入?summarise 。 您还可以从此数据集中提取哪些其他摘要信息？ 帮助我们改进本节课程 此处应插入文本和反馈二维码或问卷链接 "],["数据可视化.html", "Chapter 4 数据可视化 概述 ggplot2 的基础知识 你的第一次可视化 散点图 绘制直方图 NHANES 数据集 附加资源 帮助我们改进本节课程", " Chapter 4 数据可视化 概述 能够构建清晰的可视化是将数据成功传达给目标受众的关键。 最近有几本专注于数据可视化的优秀书籍，我建议您看一下。 它们都提供了关于数据可视化的精彩视角，并且充满了不同类型的数据可视化的精彩示例，您将在本次研讨会中学习如何构建其中一些示例。 如果您单击 Claus Wilke 书的图像，您将看到该书的在线版本（显然是用 R 编写的！） 首先，我希望您观看这个简短的视频，其中我给出了一些可以在 R 中构建的数据可视化类型的示例，以及为什么您可能希望避免构建条形图。 ggplot2 的基础知识 首先我们需要加载 ggplot2 包。 由于它是 Tidyverse 的一部分（并且我们可能会与 ggplot2 一起使用其他 Tidyverse 包），因此我们使用library(tidyverse)代码行将其加载到我们的库中。 library(tidyverse) 在视频中，我提到使用 ggplot2 需要我们指定一些核心信息才能构建可视化。 其中包括要绘制的原始数据、表示数据的几何形状的几何图形（或几何图形）以及几何图形和对象的美观性，例如颜色、大小、形状和位置。 你的第一次可视化 下面是一个示例，我们使用mpg数据集（这是一个包含汽车信息的数据集）来构建可视化，在 y 轴上绘制与城市燃油经济性 ( cty ) 相对应的点，在 x 轴上绘制与manufacturer相对应的点轴。 mpg %&gt;% ggplot(aes(x = manufacturer, y = cty)) + geom_point() 所以，这不太好。 x 轴标签很难阅读，而且各个点看起来也不那么令人愉悦。 我们可以使用str_to_title()函数将制造商标签更改为标题大小写，并使用theme()函数轻松调整轴标签。 请注意， ggplot()代码行之间的+符号相当于%&gt;%运算符。 由于历史原因（基本上，因为ggplot()出现在 Tidyverse 中的其他包之前），在向ggplot()可视化添加图层时需要使用+ 。 mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = manufacturer, y = cty)) + geom_point() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = .5)) 改善图表 因此，让我们做一些更多的整理工作 - 我们将使用geom_jitter()函数稍微抖动点（这样它们就不会垂直堆叠），并使用labs()函数整理轴标题以显式添加轴标签（而不仅仅是使用我们数据集中的标签）。 我们还添加了一些其他调整 - 你能发现它们吗？ mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = manufacturer, y = cty)) + geom_jitter(width = .2, alpha = .75, size = 2) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = .5)) + theme(text = element_text(size = 13)) + labs(title = &quot;City Fuel Economy by Car Manufacturer&quot;, x = &quot;Manufacturer&quot;, y = &quot;City Fuel Economy (mpg)&quot;) 添加汇总统计信息可能会对我们有所帮助，例如每个汽车制造商的平均燃油经济性和平均值的置信区间。 添加摘要统计数据 我们需要添加Hmisc包以允许我们使用stat_summary()函数。 library(Hmisc) mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = manufacturer, y = cty)) + stat_summary(fun.data = mean_cl_boot, colour = &quot;black&quot;, size = 1) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = .5)) + theme(text = element_text(size = 13)) + labs(title = &quot;City Fuel Economy by Car Manufacturer&quot;, x = &quot;Manufacturer&quot;, y = &quot;City Fuel Economy (mpg)&quot;) 完成的（？）图表 目前，x 轴按字母顺序排序。 我们重新排序它怎么样，以便它从平均燃油经济性最高的制造商到最低的制造商。 另外，我们翻转可视化以便交换轴并添加一些其他调整怎么样？ mpg %&gt;% mutate(manufacturer = str_to_title(manufacturer)) %&gt;% ggplot(aes(x = fct_reorder(manufacturer, .fun = mean, cty), y = cty, colour = manufacturer)) + stat_summary(fun.data = mean_cl_boot, size = 1) + geom_jitter(alpha = .25) + theme_minimal() + theme(text = element_text(size = 13)) + labs(title = &quot;Manufacturer by City Fuel Economy&quot;, x = &quot;Manufacturer&quot;, y = &quot;City Fuel Economy (mpg)&quot;) + guides(colour = &#39;none&#39;) + coord_flip() 这看起来很不错。 你能说出我添加的其他代码的作用吗？ 尝试更改一些数字，看看会发生什么。 您可以通过在某行代码前面添加#来阻止该行代码运行。 因此，如果您需要暂时不运行某行，只需添加#（快捷键: win—ctrl+shift+c, mac—command+shift+c）而不是删除该行。 作图很少会完全“完成”，因为您经常会想到可能会做出一些改进的细微美学调整。 使用facet_wrap() 我们可能认为燃油经济性随着车辆类型（例如，跑车可能比中型汽车更耗油）和发动机数量大小（例如，具有更大发动机的汽车可能更耗油）而变化。 在下面的可视化中，我们将使用facet_wrap()函数为我们要分面的因素的每个级别构建单独的可视化（忽略 SUV）。 mpg %&gt;% filter(class != &quot;suv&quot;) %&gt;% mutate(class = str_to_title(class)) %&gt;% ggplot(aes(x = displ, y = cty, colour = class)) + geom_jitter(width = .2) + theme_minimal() + theme(text = element_text(size = 13)) + labs(title = &quot;City Fuel Economy by Engine Displacement&quot;, x = &quot;Engine Displacement (litres)&quot;, y = &quot;City Fuel Economy (mpg)&quot;) + guides(colour = &#39;none&#39;) + facet_wrap(~ class) 你能说出每段代码在做什么吗？ 再次编辑数字并在要暂时忽略的行前添加# ，看看会发生什么。 散点图 上面我们重点在一个轴上绘制数值变量，在另一轴上绘制分类变量。 在某些情况下，我们想要创建散点图，使我们能够绘制两个数值变量的相互关系 - 可能是为了确定两者之间是否存在关系。 下面我们在 y 轴上绘制发动机排量，在 x 轴上绘制城市燃油经济性。 mpg %&gt;% mutate(class = str_to_upper(class)) %&gt;% ggplot(aes(x = cty, y = displ)) + geom_point(aes(colour = class)) + geom_smooth(se = FALSE) + theme(text = element_text(size = 13)) + theme_minimal() + labs(x = &quot;City Fuel Economy (mpg)&quot;, y = &quot;Engine Displacement (litres)&quot;, colour = &quot;Vehicle Class&quot;) 在上面的示例中，我们使用geom_smooth()函数添加一个对应于将曲线拟合到数据的图层。 我们可以看到，对于小于 25 英里/加仑的燃油经济性值，发动机排量和燃油经济性之间存在相当明显的负相关性，但对于大于 25 英里/加仑的值，两者之间几乎没有关系。 这些似乎表明，有些发动机相对较小的汽车具有很好的燃油经济性，而其他具有类似发动机尺寸的汽车的燃油经济性要差得多。 绘制直方图 我们可能想要绘制发动机尺寸的直方图（以升为单位测量并在mpg数据集中的变量displ中捕获）以了解该变量的分布情况。 mpg %&gt;% ggplot(aes(x = displ)) + geom_histogram(binwidth = .5, fill = &quot;grey&quot;) + labs(title = &quot;Histogram of Engine Displacement&quot;, x = &quot;Engine Displacement (litres)&quot;, y = &quot;Count&quot;) ggridges 包 鉴于在之前的可视化中，我们看到车辆类别之间似乎存在差异，如果我们能够比较按车辆类别划分的发动机尺寸的分布，不是很好吗？ 我们现在将使用ggridges()包来做到这一点…… library(ggridges) mpg %&gt;% mutate(class = str_to_title(class)) %&gt;% ggplot(aes(x = displ, y = fct_reorder(class, .fun = mean, displ))) + geom_density_ridges(height = .5, aes(fill = class)) + theme_minimal() + theme(text = element_text(size = 13)) + guides(fill = &#39;none&#39;) + labs(x = &quot;Engine Displacement (litres)&quot;, y = NULL) NHANES 数据集 我们现在将可视化 NHANES 数据集的各个方面。 这是美国国家健康统计中心（NCHS）收集的调查数据，该中心自 1960 年代初以来进行了一系列健康和营养调查。 自 1999 年以来，每年约有 5,000 名各个年龄段的人在家中接受采访，并完成调查的健康检查部分。 健康检查在移动检查中心（MEC）进行。 NHANES 的目标人群是“美国的非制度化平民居民人口”。 NHANES（美国国家健康和营养检查调查）使用复杂的调查设计（参见 http://www.cdc.gov/nchs/data/series/sr_02/sr02_162.pdf），对某些亚群体（如少数族裔）进行过度抽样。 对原始 NHANES 数据的简单分析可能会导致错误的结论。 例如，数据中每个种族群体的人口百分比与他们在人口中的百分比有很大不同。 我们需要加载NHANES包，因为这是包含数据集的地方。 library(NHANES) 如果运行上述命令时出现错误，是因为您还没有使用install.packages(\"NHANES\")在您的计算机上安装该软件包吗？ 首先，我们将探索 NHANES 数据集。 ncol(NHANES) ## [1] 76 nrow(NHANES) ## [1] 10000 我们看到有 76 列和 10,000 行。 如果我们使用函数head()我们可以看到数据帧的前几行。 head(NHANES) ## # A tibble: 6 × 76 ## ID SurveyYr Gender Age AgeDecade AgeMonths Race1 Race3 Education ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 51624 2009_10 male 34 &quot; 30-39&quot; 409 White &lt;NA&gt; High School ## 2 51624 2009_10 male 34 &quot; 30-39&quot; 409 White &lt;NA&gt; High School ## 3 51624 2009_10 male 34 &quot; 30-39&quot; 409 White &lt;NA&gt; High School ## 4 51625 2009_10 male 4 &quot; 0-9&quot; 49 Other &lt;NA&gt; &lt;NA&gt; ## 5 51630 2009_10 female 49 &quot; 40-49&quot; 596 White &lt;NA&gt; Some College ## 6 51638 2009_10 male 9 &quot; 0-9&quot; 115 White &lt;NA&gt; &lt;NA&gt; ## # ℹ 67 more variables: MaritalStatus &lt;fct&gt;, HHIncome &lt;fct&gt;, HHIncomeMid &lt;int&gt;, ## # Poverty &lt;dbl&gt;, HomeRooms &lt;int&gt;, HomeOwn &lt;fct&gt;, Work &lt;fct&gt;, Weight &lt;dbl&gt;, ## # Length &lt;dbl&gt;, HeadCirc &lt;dbl&gt;, Height &lt;dbl&gt;, BMI &lt;dbl&gt;, ## # BMICatUnder20yrs &lt;fct&gt;, BMI_WHO &lt;fct&gt;, Pulse &lt;int&gt;, BPSysAve &lt;int&gt;, ## # BPDiaAve &lt;int&gt;, BPSys1 &lt;int&gt;, BPDia1 &lt;int&gt;, BPSys2 &lt;int&gt;, BPDia2 &lt;int&gt;, ## # BPSys3 &lt;int&gt;, BPDia3 &lt;int&gt;, Testosterone &lt;dbl&gt;, DirectChol &lt;dbl&gt;, ## # TotChol &lt;dbl&gt;, UrineVol1 &lt;int&gt;, UrineFlow1 &lt;dbl&gt;, UrineVol2 &lt;int&gt;, … 整理数据 看起来有一些参与者在数据集中出现了多次 - 这可能是由于描述中提到的过采样 - 前几行都是参与者ID 51624的数据。我们可以使用select()函数和n_distinct()函数来告诉我们数据集中唯一ID的数量。 NHANES %&gt;% select(ID) %&gt;% n_distinct() ## [1] 6779 我们看到我们有6,779个独特的个体。让我们整理我们的数据，以删除重复的ID。请注意，下面我们使用了管道运算符%&gt;%，你可以将其理解为’然后’，所以它意味着我们正在使用NHANES数据集，然后筛选出具有不同ID号码的行。管道运算符确实有助于提高数据整理代码的可读性，并且是整洁宇宙哲学的重要组成部分-整洁数据和整洁代码。 NHANES_tidied &lt;- NHANES %&gt;% distinct(ID, .keep_all = TRUE) ncol(NHANES_tidied) ## [1] 76 nrow(NHANES_tidied) ## [1] 6779 好的，所以我们整理好的数据集被赋给了变量NHANES_tidied，并且有6,779行（但仍然有76列）- 这是我们预期的，因为我们有6,779个独特的个体。 绘制直方图 让我们开始探索数据吧。我们有很多潜在的变量和关系可以探索。我看到我们有一个标记为教育的因子。我们还有与健康相关的信息，比如BMI - 首先让我们绘制一个BMI的直方图。 NHANES_tidied %&gt;% ggplot(aes(x = BMI)) + geom_histogram(bins = 100, na.rm = TRUE) 我们在这里看到一个相当向右倾斜的分布。请注意我们对na.rm参数的使用-这个参数在许多tidyverse函数中都出现，并且通过将其设置为TRUE，我们告诉R忽略数据框中任何存在缺失数据的部分（由NA表示）。 总结统计数据 BMI是否随教育水平的不同而变化？ 在下面的代码中，我们使用存储在变量NHANES_tidied中的数据，按教育水平进行分组，然后进行汇总，以生成每个组的中位数BMI。同样，这次我们使用na.rm = TRUE参数和summarise()函数来从计算中删除任何缺失值（NA）。 NHANES_tidied %&gt;% group_by(Education) %&gt;% summarise(median = median(BMI, na.rm = TRUE)) ## # A tibble: 6 × 2 ## Education median ## &lt;fct&gt; &lt;dbl&gt; ## 1 8th Grade 28.6 ## 2 9 - 11th Grade 28.2 ## 3 High School 28.2 ## 4 Some College 28.4 ## 5 College Grad 26.5 ## 6 &lt;NA&gt; 18.9 看起来受过大学教育的人具有最低的中位数BMI（忽略了NA类别，该类别对应于我们没有记录教育水平的情况）。 geom_violin() 让我们来画图吧！请注意，在这里我们会过滤掉没有记录BMI值的情况。函数is.na()在应用于缺失数据(NA)的情况下返回TRUE - 我们使用!运算符来否定这一点，并使用逻辑与运算符&amp;将这些表达式组合在一起。 下面一行以filter()开头的代码意味着筛选出教育和BMI都不缺失的情况。这意味着传递给ggplot()函数的NHANES_tidied数据在我们感兴趣的关键变量上没有缺失数据。 然后我添加一个geom_violin()图层来捕捉每个教育水平的分布形状，并添加一个geom_boxplot()图层来为我们的教育因素创建一个箱线图。 调用guides(colour = 'none')会抑制显示颜色图例 - 在其前面加上#并重新运行代码以查看变化。 NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI)) %&gt;% ggplot(aes(x = Education, y = BMI, colour = Education)) + geom_violin() + geom_jitter(alpha = .2, width = .1) + geom_boxplot(alpha = .5) + guides(colour = &#39;none&#39;) + labs(title = &quot;Examining the effect of education level on BMI&quot;, x = &quot;Education Level&quot;, y = &quot;BMI&quot;) 绘制交互效应 我们还可以绘制两个因素之间的相互作用。对于上面的图，我们将添加一个因素Diabetes（有两个水平 - Yes vs. No）来看看它如何与教育水平相互作用。为了捕捉这种相互作用的性质，我们在指定x轴美学时使用表达式Education:Diabetes。请注意，我已将x轴标签旋转45度以便更容易阅读。 NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI) &amp; !is.na(Diabetes)) %&gt;% ggplot(aes(x = Education:Diabetes, y = BMI, colour = Education)) + geom_violin() + geom_jitter(alpha = .2, width = .1) + geom_boxplot(alpha = .5) + guides(colour = &#39;none&#39;) + theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) + labs(title = &quot;Examining the effect of education level and diabetes on BMI&quot;, x = &quot;Education Level x Diabetes&quot;, y = &quot;BMI&quot;) 我们可以从上面的图表看到，无论教育水平如何，患有糖尿病的人似乎也有更高的BMI得分。 使用facet_wrap()绘制直方图 我们还可以根据每个教育水平单独绘制BMI的直方图 - 我们使用facet_wrap()函数来实现这一点。 NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI)) %&gt;% group_by(Education) %&gt;% ggplot(aes(x = BMI, fill = Education)) + geom_histogram() + guides(fill = &#39;none&#39;) + labs(title = &quot;Examining the effect of education level on BMI&quot;, x = &quot;BMI&quot;, y = &quot;Number of cases&quot;) + facet_wrap(~ Education) 在上面的图中，注意每个图使用相同的y轴刻度 - 这使得比较有点棘手，因为每个教育水平的案例数量不同。在facet_wrap()行的Education之后添加以下scales = \"free\"。有什么变化？ 我们可以使用密度函数生成直方图，而不是使用计数。让我们还加上一个密度曲线。 NHANES_tidied %&gt;% filter(!is.na(Education) &amp; !is.na(BMI)) %&gt;% group_by(Education) %&gt;% ggplot(aes(x = BMI, fill = Education)) + geom_histogram(aes(y = ..density..)) + geom_density(aes(y = ..density..)) + guides(fill = &#39;none&#39;) + labs( title = &quot;Examining the effect of education level on BMI&quot;, x = &quot;BMI&quot;, y = &quot;Density&quot;) + facet_wrap(~ Education) 作业 使用mpg或NHANES数据集创建一些新的可视化。您可以使用任何数据集构建许多不同的可视化。欢迎在我们的微信群中发布您的作品。 附加资源 如果您对了解更多关于ggplot2的内容感兴趣，您可能会对观看Thomas Lin Pedersen的这两个（非常）详细的网络研讨会感兴趣（Youtube视频）。Thomas Lin Pedersen是ggplot2及其相关软件包的开发人员之一。 视频一可以点击这里，视频二可以点击这里 帮助我们改进本节课程 此处应插入文本和反馈二维码或问卷链接 "],["r-markdown.html", "Chapter 5 R Markdown 概述 作业 帮助我们改进本节课程", " Chapter 5 R Markdown 在本次研讨会中，我将向您展示如何使用 R Markdown 生成 .html 格式的报告。 使用 R Markdown 编写的报告允许您将编写的叙述与 R 代码块以及与这些代码块相关的输出全部合并到一个knitted文档中。 本单元的作业需要使用 R Markdown 生成。 概述 在这个研讨会中，我们将简要介绍如何使用R Markdown生成包含叙述、代码和代码输出的报告。 在下面的视频中，我将简要介绍如何将您编写的R脚本转换为R Markdown文档，您可以将其“编织”并与他人共享。 有很多资源可帮助您探索 R Markdown 的各种可能性。一个很好的起点是 Yihui Xie、J. J. Allaire 和 Garrett Grolemund 的《R Markdown: The Definitive Guide》。只需点击这个链接即可进入该书的在线版本。 您可能还对 Yihui Xie、Christophe Dervieux 和 Emily Riederer 的“R Markdown Cookbook”感兴趣。再次，只需点击这个链接即可进入该书的在线版本。 作业 拿出你已经写好的脚本之一 - 也许是你开发的可视化脚本之一 - 并使用它创建一个 R Markdown 文档。将你的脚本代码添加到一个新的 R Markdown 文档中，以有意义的小代码块的形式（每个代码块可能只有几行代码） - 你可能有一个代码块来加载你的库，一个代码块来读取你的数据，另一个代码块用于整理你的数据，然后为你编写的每个可视化部分分别创建代码块。在每个小代码块之前，添加一些解释以下代码块的叙述。调整每个代码块开头的消息和警告参数，以便警告和消息不会显示在最终生成的 R Markdown .html文件中。 记住，当你编写一个 R Markdown 文档时，它是在它自己的 R 会话中工作的，因此无法访问你编写 R Markdown 文档所在的主要 R 会话中的任何内容。这意味着你的 R Markdown 必须在文档内部加载所需的库并读取数据。 这个单元中的所有内容都是用R Markdown编写的 - 所以如果你想看看我是如何做到的，你可以点击这个链接，然后查看相关GitHub仓库上脚本文件夹中的.Rmd文件。 帮助我们改进本节课程 此处应插入文本和反馈二维码或问卷链接 "],["普通线性模型---回归-第一部分.html", "Chapter 6 普通线性模型 - 回归 第一部分 概述 简单线性回归 帮助我们改进本节课程", " Chapter 6 普通线性模型 - 回归 第一部分 在本次研讨会中，我们将在一般线性模型 (GLM) 的背景下探索简单回归。 您还将有机会构建一些回归模型，在其中根据一个预测变量预测结果变量。 您还将学习如何运行模型诊断，以确保您不会违反回归的任何关键假设。 概述 首先，我想让您观看以下视频，这些视频从复习相关性的基础知识开始，然后再介绍我们如何构建回归模型。 Dataset1.csv可以在这里下载 简单线性回归 在观看了上面的视频之后，我希望你能够通过下面的一个简单线性回归的例子在R中进行实践。记得创建一个新的.Rproj文件来保持事物的有序。 我们需要的R包 首先，我们需要安装所需的软件包。我们将安装tidyverse软件包以及其他几个软件包。软件包Hmisc允许我们使用rcorr()函数来计算Pearson’s r，而performance软件包则允许我们测试我们的模型假设。请记住，如果您之前没有在笔记本电脑上安装这些软件包，您首先需要在控制台中输入install.packages(\"packagename\")才能调用该软件包的library()函数。您可能还需要安装see软件包才能使performance软件包正常工作。如果需要，请在控制台中输入install.packages(\"see\")进行安装。 library(tidyverse) library(Hmisc) library(performance) 导入数据 导入名为crime_dataset.csv的数据集 - 这个数据集包含美国城市的人口数据、房价指数数据和犯罪数据。数据包可以点击这里下载 我们可以使用函数head()来显示我们名为“crime”的数据集的前几行。 crime &lt;- read_csv(&quot;Dataset/crime_dataset.csv&quot;) head(crime) ## # A tibble: 6 × 9 ## Year index_nsa `City, State` Population `Violent Crimes` Homicides Rapes ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1975 41.1 Atlanta, GA 490584 8033 185 443 ## 2 1975 30.8 Chicago, IL 3150000 37160 818 1657 ## 3 1975 36.4 Cleveland, OH 659931 10403 288 491 ## 4 1975 20.9 Oakland, CA 337748 5900 111 316 ## 5 1975 20.4 Seattle, WA 503500 3971 52 324 ## 6 NA NA &lt;NA&gt; NA NA NA NA ## # ℹ 2 more variables: Assaults &lt;dbl&gt;, Robberies &lt;dbl&gt; 整理数据 首先让我们进行一些整理。有一列将城市和州的信息合并在一起。让我们使用函数separate()将该信息分离成两个新列，分别称为“City”和“State”。我们还将重命名列，将“index_nsa”列的名称更改为“House_price”，并去掉“Violent Crimes”标题中的空格。 crime_tidied &lt;- crime %&gt;% separate(col = &quot;City, State&quot;, into = c(&quot;City&quot;, &quot;State&quot;)) %&gt;% rename(House_price = index_nsa) %&gt;% rename(Violent_Crimes = &quot;Violent Crimes&quot;) head(crime_tidied) ## # A tibble: 6 × 10 ## Year House_price City State Population Violent_Crimes Homicides Rapes ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1975 41.1 Atlanta GA 490584 8033 185 443 ## 2 1975 30.8 Chicago IL 3150000 37160 818 1657 ## 3 1975 36.4 Cleveland OH 659931 10403 288 491 ## 4 1975 20.9 Oakland CA 337748 5900 111 316 ## 5 1975 20.4 Seattle WA 503500 3971 52 324 ## 6 NA NA &lt;NA&gt; &lt;NA&gt; NA NA NA NA ## # ℹ 2 more variables: Assaults &lt;dbl&gt;, Robberies &lt;dbl&gt; 绘制数据 我们可能首先认为随着人口规模的增加，犯罪率也会增加。让我们先建立一个散点图。 crime_tidied %&gt;% ggplot(aes(x = Population, y = Violent_Crimes)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Population&quot;, y = &quot;Violent Crimes&quot;) 皮尔逊相关系数(Pearson’s r) 这个图看起来相当有趣。怎么样计算皮尔逊相关系数？ rcorr(crime_tidied$Population, crime_tidied$Violent_Crimes) ## x y ## x 1.00 0.81 ## y 0.81 1.00 ## ## n ## x y ## x 1714 1708 ## y 1708 1708 ## ## P ## x y ## x 0 ## y 0 看看r值和p值 - r = .81，p &lt; .001。因此，我们的暴力犯罪变量中约64%的变异（ variance）可以通过我们的人口大小变量来解释。很明显，人口规模和暴力犯罪率之间存在正相关关系。从图中可以看出，我们可能会得出结论，这种关系受到一小部分非常大城市犯罪的过度影响（图上方的右上角）。让我们排除人口超过2,000,000的城市。 crime_filtered &lt;- filter(crime_tidied, Population &lt; 2000000) 现在让我们重新绘制图表。由于可能仍然有很多点（因此会出现很多点大致出现在同一位置），我们可以在代码的geom_point()行中将alpha参数设置为&lt;1。该参数对应每个点的透明度。将其更改为其他值以查看发生了什么。 crime_filtered %&gt;% ggplot(aes(x = Population, y = Violent_Crimes)) + geom_point(alpha = .25) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Population&quot;, y = &quot;Violent Crimes&quot;) 并计算皮尔逊相关系数。 rcorr(crime_filtered$Population, crime_filtered$Violent_Crimes) ## x y ## x 1.00 0.69 ## y 0.69 1.00 ## ## n ## x y ## x 1659 1653 ## y 1653 1653 ## ## P ## x y ## x 0 ## y 0 仍然存在明显的正相关关系（r = 0.69）。让我们建立一个线性模型。数据集包含大量数据，每个城市每年都会出现多次。对于我们的线性模型，我们的观察结果需要相互独立，所以让我们只关注2015年。这样每个城市只会出现一次。 首先我们应用我们的过滤器。 crime_filtered &lt;- filter(crime_filtered, Year == 2015) 然后我们构建一个图。我使用geom_text()图层来绘制城市名称，并将check_overlap参数设置为TRUE以确保标签不重叠。 crime_filtered %&gt;% ggplot(aes(x = Population, y = Violent_Crimes, label = City)) + geom_point() + geom_text(nudge_y = 500, check_overlap = TRUE) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + xlim(0, 1800000) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Population&quot;, y = &quot;Violent Crimes&quot;) 这显示了一个明显的正线性关系，所以让我们计算一下皮尔逊相关系数。 rcorr(crime_filtered$Population, crime_filtered$Violent_Crimes) ## x y ## x 1.00 0.65 ## y 0.65 1.00 ## ## n ## x y ## x 42 40 ## y 40 40 ## ## P ## x y ## x 0 ## y 0 建模数据 想象我们是城市规划师，我们想知道暴力犯罪可能随人口规模增加而增加的程度。换句话说，我们想弄清楚暴力犯罪率是如何由人口规模预测出来的。 我们将构建两个线性模型 - 第一个model1 ，我们使用结果变量的平均值作为预测变量，第二个model2 ，我们使用人口规模来预测暴力犯罪结果。 model1 &lt;- lm(Violent_Crimes ~ 1, data = crime_filtered) model2 &lt;- lm(Violent_Crimes ~ Population, data = crime_filtered) 检查我们的假设 让我们使用performance包中的check_model()函数来检查我们模型的假设。 check_model(model2) 我们的数据集很小，所以我们的一些诊断图看起来不太好。稍后我们会回到有影响力的异常值（第29个案例），但现在让我们使用anova()函数来查看以人口作为预测变量的模型是否比仅使用均值的模型更好。 anova(model1, model2) ## Analysis of Variance Table ## ## Model 1: Violent_Crimes ~ 1 ## Model 2: Violent_Crimes ~ Population ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 39 445568991 ## 2 38 257690819 1 187878173 27.705 5.813e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 这是 - 模型不同，你会看到残差平方和（或误差）在第二个模型中较小（该模型以人口为预测变量）。这意味着我们观测数据与回归线模型model2之间的偏差明显小于我们观测数据与平均值作为我们数据模型model1之间的偏差。所以让我们得到model2的参数估计。 解读我们的模型 summary(model2) ## ## Call: ## lm(formula = Violent_Crimes ~ Population, data = crime_filtered) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5465.8 -1633.4 -809.1 684.3 6213.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.443e+02 9.216e+02 1.025 0.312 ## Population 6.963e-03 1.323e-03 5.264 5.81e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2604 on 38 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.4217, Adjusted R-squared: 0.4064 ## F-statistic: 27.71 on 1 and 38 DF, p-value: 5.813e-06 截距对应于我们的回归线与y轴相交的地方，人口参数对应于我们线的斜率。我们可以看到，每增加1个人口，暴力犯罪率就会额外增加0.006963。 对于一个人口约一百万的城市，将会有大约7907起暴力犯罪。我们通过将我们的预测值（0.006963）乘以1,000,000，然后加上截距（944.3）来计算得出这个结果。这给我们7907.3起犯罪案件 - 这与我们在上面的回归线上看到的情况相符。我们可能有一些异常值 - 你如何找出这些异常值？尝试排除你发现的任何异常值并重新构建你的模型。 作业 您现在有三个任务： 1. 检查2015年人口规模和抢劫案件之间是否存在相同的关系。 2. 2015年暴力犯罪数量是否预测了房价？ 3. 2015年人口规模是否预测了房价？ 帮助我们改进本节课程 此处应插入文本和反馈二维码或问卷链接 "],["普通线性模型---回归-第二部分.html", "Chapter 7 普通线性模型 - 回归 第二部分 概述 多元回归 逐步回归 帮助我们改进本节课程", " Chapter 7 普通线性模型 - 回归 第二部分 在本次研讨会中，我们将探讨一般线性模型 (GLM) 背景下的多元回归。 多重回归建立在简单回归的基础上，不同之处在于，只有一个预测变量（就像简单回归的情况），我们将处理多个预测变量。 同样，您将有机会构建一些回归模型并使用各种方法来决定哪一个是“最好的”。 您还将学习如何对这些模型运行模型诊断，就像在简单回归中所做的那样。 概述 首先，我想让您观看以下视频，该视频是第一个回归工作坊的延续。我们将探讨如何使用R中的lm()函数构建具有多个预测变量的回归模型，测试我们的模型假设，并解释输出结果。我们将研究使用多个预测变量构建逐步回归模型的不同方法，最后讨论中介和调节。 多元回归 在标准多元回归中，所有自变量（independent variables, IVs）都被输入到方程中，并同时进行评估其贡献。让我们通过一个具体的例子来进行讲解。 教育心理学家进行了一项研究，调查了7至9岁的小学生的拼写能力与心理语言学变量之间的关系。研究人员向孩子们展示了48个单词，这些单词根据一定的特征进行了系统变化，如掌握年龄、词频、词长和形象性。心理学家想要检查测试结果是否准确反映了孩子们的拼写能力，这是通过标准化拼写测试来估计的。也就是说，心理学家想要检查她的测试是否合适。 儿童的实际年龄（以月为单位）（年龄），他们的阅读年龄（reading age, RA），他们的标准化阅读年龄（standardised reading age, std_RA）以及他们的标准化拼写分数（standardised spelling score, std_SPELL）被选择为预测变量。准则变量（Y）是每个孩子使用48个单词列表时达到的正确拼写百分比（percentage correct spelling, corr_spell）得分。 首先，我们需要加载我们需要的软件包 - require函数假设它们已经在您的计算机上。如果它们不在，那么您首先需要install.packages (“packagename”) ： 我们需要的包 library(tidyverse) # Load the tidyverse packages library(Hmisc) # Needed for correlation library(MASS) # Needed for maths functions library(car) # Needed for VIF calculation library(olsrr) # Needed for stepwise regression library(performance) # Needed to check model assumptions 导入数据 现在您需要读取数据文件。数据文件可以点击这里下载。 MRes_tut2 &lt;- read_csv(&quot;Dataset/MRes_tut2.csv&quot;) 检查可能的关系 在我们开始之前，让我们来看一下我们的自变量IVs（预测因子，predictors）和因变量DV（结果，outcome）之间的关系。我们可以绘制图表来描述它们之间的相关性。我们将依次绘制测试表现与我们的四个预测因子之间的关系图： ggplot(MRes_tut2, aes(x = age, y = corr_spell)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) ggplot(MRes_tut2, aes(x = RA, y = corr_spell)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) ggplot(MRes_tut2, aes(x = std_RA, y = corr_spell)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) ggplot(MRes_tut2, aes(x = std_SPELL, y = corr_spell)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_minimal() + theme(text = element_text(size = 13)) 对数据建模 我们首先将进行分层回归 - 我们将构建一个模型（我们将其称为model0），该模型是我们结果变量的平均值，另一个模型（model1）包含所有的预测变量： model0 &lt;- lm(corr_spell ~ 1, data = MRes_tut2) model1 &lt;- lm(corr_spell ~ age + RA + std_RA + std_SPELL, data = MRes_tut2) 让我们将它们相互比较： anova(model0, model1) ## Analysis of Variance Table ## ## Model 1: corr_spell ~ 1 ## Model 2: corr_spell ~ age + RA + std_RA + std_SPELL ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 46 26348.4 ## 2 42 3901.1 4 22447 60.417 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 我们看到这些模型彼此不同（查看比较的p值），并且具有四个预测变量的模型具有较低的残差（RSS）值，这意味着模型与观测数据之间的误差相对于较简单的仅有截距的模型（即均值）和观测数据较小。 检查我们的假设 好的，所以它们是不同的 - 现在让我们来绘制关于我们模型假设的信息 - 记住，我们对我们的案例的Cook’s distance值特别感兴趣… check_model(model1) 误差（errors）在我们拟合的值上分布相对均匀（同方差性） - 尽管对于高拟合值而言稍微糟糕一些 - 从 Q-Q 图中我们可以看出它们看起来相当正常（它们应该遵循对角线）。那么有关有影响力的案例呢？所以，第 10 个案例看起来有点可疑 - 它有一个较高的 Cook 距离值 - 这意味着它对我们的模型产生了不成比例的影响。让我们使用 filter() 函数将其排除 - 符号 != 表示“不等于”，因此我们选择除了第 10 个案例以外的值。 放弃一个有影响力的案例(Influential Case) MRes_tut2_drop10 &lt;- filter(MRes_tut2, case != &quot;10&quot;) 重新建模数据 我们现在创建另一个模型（model2），不包括 Case 10。 model2 &lt;- lm(corr_spell ~ age + RA + std_RA + std_SPELL, data = MRes_tut2_drop10) 让我们再次使用check_model()检查模型假设。 检查我们的假设 check_model(model2) 现在，让我们来看一下由VIF测量得出的多重共线性值： vif(model2) ## age RA std_RA std_SPELL ## 3.843462 14.763168 14.672084 3.140457 看起来 RA 和 std_RA 有问题。我们可以使用 rcorr() 函数来研究它们之间的相关性： rcorr(MRes_tut2_drop10$RA, MRes_tut2_drop10$std_RA) ## x y ## x 1.00 0.88 ## y 0.88 1.00 ## ## n= 46 ## ## ## P ## x y ## x 0 ## y 0 重新建模数据 相关性很高（0.88），所以让我们排除具有最高VIF值的预测变量（即RA），并构建一个新模型： model3 &lt;- lm(corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10) vif(model3) ## age std_RA std_SPELL ## 1.190827 2.636186 2.821235 检查我们的假设 这些值现在看起来还不错。让我们再次检查模型假设。 check_model(model3) 总结模型 现在让我们生成系数（ coefficients），因为这看起来像一个合理的模型。 summary(model3) ## ## Call: ## lm(formula = corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.801 -6.907 1.327 5.155 24.669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -209.4400 26.7210 -7.838 9.43e-10 *** ## age 1.1033 0.2133 5.172 6.09e-06 *** ## std_RA 0.3804 0.1385 2.747 0.00883 ** ## std_SPELL 1.2107 0.1650 7.339 4.78e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.824 on 42 degrees of freedom ## Multiple R-squared: 0.8388, Adjusted R-squared: 0.8273 ## F-statistic: 72.87 on 3 and 42 DF, p-value: &lt; 2.2e-16 model0 &lt;- lm(corr_spell ~ 1, data = MRes_tut2_drop10) anova(model3, model0) ## Analysis of Variance Table ## ## Model 1: corr_spell ~ age + std_RA + std_SPELL ## Model 2: corr_spell ~ 1 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 42 4053.5 ## 2 45 25151.0 -3 -21098 72.866 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 我们将方程写成如下形式： Spelled correct = -209.44 + 1.10(age) + 0.38(std_RA) + 1.21(std_SPELL) + residual 逐步回归 我们还可以进行逐步回归 - 向前是当您从空模型开始并添加预测变量，直到它们不能解释更多方差时，向后是当您从完整模型开始并删除预测变量，直到删除开始影响模型的预测能力。 让我们保留案例 10 并删除高 VIF 预测变量 (RA)。 这对于具有大量预测变量的模型非常方便，其中顺序回归的顺序不明显。 对数据进行建模 model0 &lt;- lm(corr_spell ~ 1, data = MRes_tut2_drop10) model1 &lt;- lm(corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10) 让我们向前逐步回归： steplimitsf &lt;- step(model0, scope = list (lower = model0, upper = model1), direction = &quot;forward&quot;) ## Start: AIC=291.98 ## corr_spell ~ 1 ## ## Df Sum of Sq RSS AIC ## + std_SPELL 1 17802.2 7348.8 237.39 ## + std_RA 1 14780.1 10370.9 253.23 ## &lt;none&gt; 25151.0 291.98 ## + age 1 48.7 25102.3 293.89 ## ## Step: AIC=237.39 ## corr_spell ~ std_SPELL ## ## Df Sum of Sq RSS AIC ## + age 1 2567.20 4781.6 219.62 ## + std_RA 1 714.14 6634.7 234.69 ## &lt;none&gt; 7348.8 237.39 ## ## Step: AIC=219.62 ## corr_spell ~ std_SPELL + age ## ## Df Sum of Sq RSS AIC ## + std_RA 1 728.1 4053.5 214.02 ## &lt;none&gt; 4781.6 219.62 ## ## Step: AIC=214.02 ## corr_spell ~ std_SPELL + age + std_RA summary(steplimitsf) ## ## Call: ## lm(formula = corr_spell ~ std_SPELL + age + std_RA, data = MRes_tut2_drop10) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.801 -6.907 1.327 5.155 24.669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -209.4400 26.7210 -7.838 9.43e-10 *** ## std_SPELL 1.2107 0.1650 7.339 4.78e-09 *** ## age 1.1033 0.2133 5.172 6.09e-06 *** ## std_RA 0.3804 0.1385 2.747 0.00883 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.824 on 42 degrees of freedom ## Multiple R-squared: 0.8388, Adjusted R-squared: 0.8273 ## F-statistic: 72.87 on 3 and 42 DF, p-value: &lt; 2.2e-16 向后逐步回归： steplimitsb &lt;- step(model1, direction = &quot;back&quot;) ## Start: AIC=214.02 ## corr_spell ~ age + std_RA + std_SPELL ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 4053.5 214.02 ## - std_RA 1 728.1 4781.6 219.62 ## - age 1 2581.2 6634.7 234.69 ## - std_SPELL 1 5198.3 9251.8 249.98 summary(steplimitsb) ## ## Call: ## lm(formula = corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.801 -6.907 1.327 5.155 24.669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -209.4400 26.7210 -7.838 9.43e-10 *** ## age 1.1033 0.2133 5.172 6.09e-06 *** ## std_RA 0.3804 0.1385 2.747 0.00883 ** ## std_SPELL 1.2107 0.1650 7.339 4.78e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.824 on 42 degrees of freedom ## Multiple R-squared: 0.8388, Adjusted R-squared: 0.8273 ## F-statistic: 72.87 on 3 and 42 DF, p-value: &lt; 2.2e-16 在逐步回归中使用向前和向后回归 steplimitsboth &lt;- step(model0, scope = list (upper = model1), direction = &quot;both&quot;) ## Start: AIC=291.98 ## corr_spell ~ 1 ## ## Df Sum of Sq RSS AIC ## + std_SPELL 1 17802.2 7348.8 237.39 ## + std_RA 1 14780.1 10370.9 253.23 ## &lt;none&gt; 25151.0 291.98 ## + age 1 48.7 25102.3 293.89 ## ## Step: AIC=237.39 ## corr_spell ~ std_SPELL ## ## Df Sum of Sq RSS AIC ## + age 1 2567.2 4781.6 219.62 ## + std_RA 1 714.1 6634.7 234.69 ## &lt;none&gt; 7348.8 237.39 ## - std_SPELL 1 17802.2 25151.0 291.98 ## ## Step: AIC=219.62 ## corr_spell ~ std_SPELL + age ## ## Df Sum of Sq RSS AIC ## + std_RA 1 728.1 4053.5 214.02 ## &lt;none&gt; 4781.6 219.62 ## - age 1 2567.2 7348.8 237.39 ## - std_SPELL 1 20320.7 25102.3 293.89 ## ## Step: AIC=214.02 ## corr_spell ~ std_SPELL + age + std_RA ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 4053.5 214.02 ## - std_RA 1 728.1 4781.6 219.62 ## - age 1 2581.2 6634.7 234.69 ## - std_SPELL 1 5198.3 9251.8 249.98 检查我们的假设 check_model(steplimitsboth) 这些看起来不错。 summary(steplimitsboth) ## ## Call: ## lm(formula = corr_spell ~ std_SPELL + age + std_RA, data = MRes_tut2_drop10) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.801 -6.907 1.327 5.155 24.669 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -209.4400 26.7210 -7.838 9.43e-10 *** ## std_SPELL 1.2107 0.1650 7.339 4.78e-09 *** ## age 1.1033 0.2133 5.172 6.09e-06 *** ## std_RA 0.3804 0.1385 2.747 0.00883 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.824 on 42 degrees of freedom ## Multiple R-squared: 0.8388, Adjusted R-squared: 0.8273 ## F-statistic: 72.87 on 3 and 42 DF, p-value: &lt; 2.2e-16 您会看到在每种情况下都会得到相同的最终模型。 我们有三个重要的预测因子。 根据p值输入预测变量 我们还可以使用olsrr包中的ols_step_forward_p()函数 - 它的工作原理是根据p值输入预测变量来找到最佳模型。 olsrr包中还有其他方法。 要查看它们，请在控制台窗口中输入olsrr:: 。 pmodel &lt;- ols_step_forward_p(model1) pmodel ## ## Selection Summary ## --------------------------------------------------------------------------- ## Variable Adj. ## Step Entered R-Square R-Square C(p) AIC RMSE ## --------------------------------------------------------------------------- ## 1 std_SPELL 0.7078 0.7012 34.1439 369.9304 12.9236 ## 2 age 0.8099 0.8010 9.5442 352.1614 10.5452 ## 3 std_RA 0.8388 0.8273 4.0000 346.5624 9.8241 ## --------------------------------------------------------------------------- 在这种情况下，具有最低 AIC 值的模型也是通过基于p值的顺序过程统计得出的模型 - 但情况可能并不总是如此。 帮助我们改进本节课程 此处应插入文本和反馈二维码或问卷链接 "],["一般线性模型---方差分析第-1-部分.html", "Chapter 8 一般线性模型 - 方差分析第 1 部分 概述 参与者之间的方差分析（Between Participants ANOVA） 重复测量方差分析 多因子方差分析（factorial ANOVA） 作业 答案 帮助我们改进本节课程", " Chapter 8 一般线性模型 - 方差分析第 1 部分 在本次研讨会中，我们将在 R 模型构建的背景下探索参与者设计、重复测量设计和因子设计之间的方差分析 (ANOVA)。 您将学习如何使用 {afex} 包构建具有 III 型平方和的模型，以及如何使用{emmeans} 包进行后续测试以探索主效应和相互作用。 概述 在本次研讨会中，我们将研究如何在 R 中进行方差分析 (ANOVA)。我们将首先探讨为什么我们倾向于使用 ANOVA（而不是多个t检验），然后继续讨论参与者之间的 ANOVA 的一些示例重复测量设计。 参与者之间的方差分析（Between Participants ANOVA） 在第二个视频中，我将向您展示如何使用{afex}包在 R 中构建参与者之间的方差分析模型。 观看完上面的视频后，就轮到您在 R 中为参与者之间的设计构建您的第一个方差分析了。请按照以下说明构建该模型。 加载我们的包 首先，我们需要加载我们将使用的三个包 - 它们是{tidyverse} 、 {afex}和{emmeans} 。 {afex}包是我们用于进行多因子方差分析（factorial ANOVA）的包。 我们使用{emmeans}包对我们将要构建的 ANOVA 模型运行后续测试。 library(tidyverse) library(afex) library(emmeans) 读取我们的数据 我们有 45 名参与者参与参与者之间的设计，我们感兴趣的是饮用的饮料对运动任务能力的影响。 我们的实验因素（饮料类型）有 3 个级别。 这些是水与单份浓缩咖啡与双份浓缩咖啡，而能力是我们在连续范围内测量的 DV。 让我们读入我们的数据。数据可以在这里下载 my_data &lt;- read_csv(&quot;Dataset/cond.csv&quot;) head(my_data) ## # A tibble: 6 × 3 ## Participant Condition Ability ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Water 4.82 ## 2 2 Water 5.41 ## 3 3 Water 5.73 ## 4 4 Water 4.36 ## 5 5 Water 5.47 ## 6 6 Water 5.50 我们看到我们有三个变量，但是我们的实验变量Condition没有被编码为因子。 让我们解决这个问题… my_data_tidied &lt;- my_data %&gt;% mutate(Condition = factor(Condition)) head(my_data_tidied) ## # A tibble: 6 × 3 ## Participant Condition Ability ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Water 4.82 ## 2 2 Water 5.41 ## 3 3 Water 5.73 ## 4 4 Water 4.36 ## 5 5 Water 5.47 ## 6 6 Water 5.50 总结我们的数据 接下来让我们进行一些汇总统计并构建数据可视化。 my_data_tidied %&gt;% group_by(Condition) %&gt;% summarise(mean = mean(Ability), sd = sd(Ability)) ## # A tibble: 3 × 3 ## Condition mean sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Double Espresso 8.89 0.467 ## 2 Single Espresso 6.99 0.419 ## 3 Water 5.17 0.362 可视化我们的数据 set.seed(1234) my_data_tidied %&gt;% ggplot(aes(x = Condition, y = Ability, colour = Condition)) + geom_violin() + geom_jitter(width = .1) + guides(colour = FALSE) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, colour = &quot;black&quot;) + theme_minimal() + theme(text = element_text(size = 13)) ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use ## &quot;none&quot; instead as of ggplot2 3.3.4. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. 我们构建了一个可视化，其中使用geom_jitter()函数绘制了原始数据点，并使用geom_violin()函数绘制了每个条件的分布形状。 我们还使用stat_summary()函数以平均值和平均值附近的置信区间的形式添加了一些汇总数据。 建立我们的方差分析模型 现在让我们使用{afex}包中的aov_4()函数构建模型。 aov_4()中方差分析模型的语法为： aov_4(DV ~ IV + (1 | Participant), data = my_data_tidied)。 ~符号表示由(1 | Participant)项预测，对应于我们的随机效应 - 我们显然无法测试世界上的所有参与者，因此仅从该群体中随机抽取样本。 最后，我们需要通过在模型的data = my_data_tidied位中明确说明我们正在使用的数据集。 我们将把aov()函数的输出映射到我称之为model变量上。 这意味着方差分析结果将存储在该变量中，并允许我们稍后访问它们。 model &lt;- aov_4(Ability ~ Condition + (1 | Participant), data = my_data_tidied) ## Contrasts set to contr.sum for the following variables: Condition 为了获得方差分析的输出，我们可以将summary()函数与我们新创建的model结合使用。 解释模型输出 summary(model) ## Anova Table (Type 3 tests) ## ## Response: Ability ## num Df den Df MSE F ges Pr(&gt;F) ## Condition 2 42 0.17484 297.05 0.93397 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 效应大小 (ges) 是广义 eta 平方，对于具有多个因子的设计，它可以作为一个有用的指标，表明每个因子（加上因子之间的任何相互作用）可以解释因变量的方差有多少。 因此，我们的模型存在影响 - F 值相当大， p值相当小），但我们尚不知道是什么导致了差异。 我们需要使用emmeans()函数进行一些成对比较，以告诉我们哪些平均值与其他平均值不同。 emmeans(model, pairwise ~ Condition) ## $emmeans ## Condition emmean SE df lower.CL upper.CL ## Double Espresso 8.89 0.108 42 8.67 9.10 ## Single Espresso 6.99 0.108 42 6.77 7.20 ## Water 5.17 0.108 42 4.95 5.38 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Double Espresso - Single Espresso 1.90 0.153 42 12.453 &lt;.0001 ## Double Espresso - Water 3.72 0.153 42 24.372 &lt;.0001 ## Single Espresso - Water 1.82 0.153 42 11.920 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates 请注意，多重比较的默认调整是 Tukey 的调整。 我们可以通过向模型添加额外参数来改变这一点，例如adjust = “bonferonni”) 。 在这种情况下，它对我们的比较没有任何影响。 emmeans(model, pairwise ~ Condition, adjust = &quot;bonferroni&quot;) ## $emmeans ## Condition emmean SE df lower.CL upper.CL ## Double Espresso 8.89 0.108 42 8.67 9.10 ## Single Espresso 6.99 0.108 42 6.77 7.20 ## Water 5.17 0.108 42 4.95 5.38 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Double Espresso - Single Espresso 1.90 0.153 42 12.453 &lt;.0001 ## Double Espresso - Water 3.72 0.153 42 24.372 &lt;.0001 ## Single Espresso - Water 1.82 0.153 42 11.920 &lt;.0001 ## ## P value adjustment: bonferroni method for 3 tests 我们发现饮料类型有显着影响（F (2,42) = 297.05， p &lt; .001，广义 η2 = .93）。 Tukey 比较显示，水组的表现显着差于单份浓缩咖啡组 ( p &lt; .001)，水组的表现显着差于双份浓缩咖啡组 ( p &lt; .001)，单份浓缩咖啡组的表现显着较差高于 Double Espresso 组 ( p &lt; .001)。 换句话说，相对于喝水，喝一些咖啡可以提高运动表现，而喝大量咖啡可以更好地提高运动表现。3 重复测量方差分析 假设我们有一个实验，要求 32 名参与者学习如何发音不同复杂程度的单词 - Very Easy、Easy、Hard 和 Very Hard。 他们在最初接触阶段就看到了这些词。 休息 30 分钟后，我们对参与者进行了测试，要求他们在计算机屏幕上出现单词时大声说出这些单词。 我们以秒为单位记录了他们的时间。 我们想知道他们对于每个单词复杂程度的响应时间是否存在差异。 读取我们的数据 首先我们读入数据。数据可以在这里下载 rm_data &lt;- read_csv(&quot;Dataset/rm_data.csv&quot;) head(rm_data) ## # A tibble: 6 × 3 ## Participant Condition RT ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Very Easy 1.25 ## 2 2 Very Easy 1.16 ## 3 3 Very Easy 1.12 ## 4 4 Very Easy 1.33 ## 5 5 Very Easy 1.16 ## 6 6 Very Easy 1.15 我们可以从head()函数中看到 Condition 尚未被编码为因子。 让我们解决这个问题。 rm_data_tidied &lt;- rm_data %&gt;% mutate(Condition = factor(Condition)) head(rm_data_tidied) ## # A tibble: 6 × 3 ## Participant Condition RT ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 Very Easy 1.25 ## 2 2 Very Easy 1.16 ## 3 3 Very Easy 1.12 ## 4 4 Very Easy 1.33 ## 5 5 Very Easy 1.16 ## 6 6 Very Easy 1.15 总结我们的数据 让我们为四个条件中的每一个生成平均值和标准差。 rm_data_tidied %&gt;% group_by(Condition) %&gt;% summarise(mean = mean(RT), sd = sd (RT)) ## # A tibble: 4 × 3 ## Condition mean sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Easy 1.23 0.0610 ## 2 Hard 1.39 0.118 ## 3 Very Easy 1.20 0.0511 ## 4 Very Hard 1.87 0.187 可视化我们的数据 并可视化数据 - 请注意，这里我使用fct_reorder()函数根据 RT 重新排序因子的级别。 这对于使我们的可视化更容易解释很有用。 rm_data_tidied %&gt;% ggplot(aes(x = fct_reorder(Condition, RT), y = RT, colour = Condition)) + geom_violin() + geom_jitter(width = .1) + guides(colour = FALSE) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, colour = &quot;black&quot;) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Condition&quot;, y = &quot;RT (s)&quot;) 建立我们的方差分析模型 我们以与之前类似的方式构建方差分析模型。 除了在这种情况下，我们的随机效应定义为(1 + Condition | Participant)。 我为了捕捉我们的Condition是重复测量因素这一事实，我们将其添加到随机效应项中，如下所示。 rm_model &lt;- aov_4(RT ~ Condition + (1 + Condition | Participant), data = rm_data_tidied) 解释模型输出 我们以与参与者之间方差分析相同的方式提取模型的摘要。 summary(rm_model) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 259.07 1 0.50313 31 15962.33 &lt; 2.2e-16 *** ## Condition 9.27 3 1.20624 93 238.23 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## Condition 0.38404 3.0211e-05 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## Condition 0.65596 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## Condition 0.7000534 3.359493e-31 使用此方式，我们没有在我们的测量中获得效应大小测量值。 不过，我们可以通过使用anova()函数要求我们的模型以 anova 格式呈现来生成它。 anova(rm_model) ## Anova Table (Type 3 tests) ## ## Response: RT ## num Df den Df MSE F ges Pr(&gt;F) ## Condition 1.9679 61.004 0.019773 238.23 0.84431 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 效应大小通过 ges 来测量，是重复测量设计的推荐效应大小测量（Bakeman，2005）。 请注意，此输出中的 dfs 始终会被纠正，就好像违反了球形度（当所有可能的受试者内条件对（即自变量的水平）之间的差异的方差不相等时，就会违反球形度） - 为保守的（为了避免 I 类错误）我们最好总是选择这些修正后的 dfs。 从这里我们可以看出我们有Condition的效果。 但我们不知道我们的因素的不同水平之间的差异在哪里。 所以我们使用emmeans()函数来找出答案。 在这里，我们将使用 Bonferroni 校正进行多重比较。 emmeans(rm_model, pairwise ~ Condition, adjust = &quot;Bonferroni&quot;) ## $emmeans ## Condition emmean SE df lower.CL upper.CL ## Easy 1.23 0.01079 31 1.21 1.25 ## Hard 1.39 0.02085 31 1.35 1.43 ## Very.Easy 1.20 0.00904 31 1.18 1.22 ## Very.Hard 1.87 0.03302 31 1.80 1.94 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Easy - Hard -0.1633 0.0226 31 -7.225 &lt;.0001 ## Easy - Very.Easy 0.0285 0.0143 31 1.986 0.3353 ## Easy - Very.Hard -0.6430 0.0338 31 -19.014 &lt;.0001 ## Hard - Very.Easy 0.1917 0.0230 31 8.354 &lt;.0001 ## Hard - Very.Hard -0.4797 0.0363 31 -13.220 &lt;.0001 ## Very.Easy - Very.Hard -0.6715 0.0341 31 -19.710 &lt;.0001 ## ## P value adjustment: bonferroni method for 6 tests 从上面我们可以看到，除了“Easy ”与“ Very Easy”的比较不重要之外，所有条件都不同于所有其他条件。 多因子方差分析（factorial ANOVA） 读取我们的数据 数据可以在这里下载 factorial_data &lt;- read_csv(&quot;Dataset/factorial_data.csv&quot;) head(factorial_data) ## # A tibble: 6 × 5 ## Subject Item RT Sentence Context ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 3 1270 Positive Negative ## 2 1 7 739 Positive Negative ## 3 1 11 982 Positive Negative ## 4 1 15 1291 Positive Negative ## 5 1 19 1734 Positive Negative ## 6 1 23 1757 Positive Negative 我们再次看到我们的两个实验因素没有被编码为因素，所以让我们解决这个问题。 factorial_data_tidied &lt;- factorial_data %&gt;% mutate(Sentence = factor(Sentence), Context = factor(Context)) head(factorial_data_tidied) ## # A tibble: 6 × 5 ## Subject Item RT Sentence Context ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 3 1270 Positive Negative ## 2 1 7 739 Positive Negative ## 3 1 11 982 Positive Negative ## 4 1 15 1291 Positive Negative ## 5 1 19 1734 Positive Negative ## 6 1 23 1757 Positive Negative 总结我们的数据 让我们生成一些汇总统计数据 - 请注意，我们在group_by()函数调用中指定了两个分组变量。 factorial_data_tidied %&gt;% group_by(Context, Sentence) %&gt;% summarise(mean_rt = mean(RT), sd_rt = sd(RT)) ## `summarise()` has grouped output by &#39;Context&#39;. You can override ## using the `.groups` argument. ## # A tibble: 4 × 4 ## # Groups: Context [2] ## Context Sentence mean_rt sd_rt ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Negative Negative 1474. 729. ## 2 Negative Positive NA NA ## 3 Positive Negative NA NA ## 4 Positive Positive 1579. 841. 我们有两个数据为NA ，表明我们的数据集中某处缺少数据。 我们现在将使用一个名为{visdat}的新包。 它允许我们使用vis_dat()函数可视化我们的数据集，并使用vis_miss()函数可视化丢失的数据。 library(visdat) vis_miss(factorial_data_tidied) 我们可以在上面的可视化中看到我们确实有一些丢失的数据。 我们需要告诉 R 我们希望它做什么。 我们可以使用na.rm = TRUE参数告诉它我们希望忽略丢失的数据。 factorial_data_tidied %&gt;% group_by(Context, Sentence) %&gt;% summarise(mean_rt = mean(RT, na.rm = TRUE), sd_rt = sd(RT, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;Context&#39;. You can override ## using the `.groups` argument. ## # A tibble: 4 × 4 ## # Groups: Context [2] ## Context Sentence mean_rt sd_rt ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Negative Negative 1474. 729. ## 2 Negative Positive 1595. 887. ## 3 Positive Negative 1633. 877. ## 4 Positive Positive 1579. 841. 现在我们有了我们期望的汇总统计数据。 可视化我们的数据 我们可以使用上面使用的ggplot()代码的修改来生成可视化。 请注意，在开始绘图之前，我正在使用filter()函数过滤丢失的数据。 我还指定我们希望使用Context:Sentence在aes()定义中绘制两个因素的组合。 我们还可以修改更多内容来改善该图的外观。 您能找出使这两个因素的标签更清晰的方法吗？ factorial_data_tidied %&gt;% filter(!is.na(RT)) %&gt;% ggplot(aes(x = Context:Sentence, y = RT, colour = Context:Sentence)) + geom_violin() + geom_jitter(width = .1, alpha = .25) + guides(colour = FALSE) + stat_summary(fun.data = &quot;mean_cl_boot&quot;, colour = &quot;black&quot;) + theme_minimal() + theme(text = element_text(size = 13)) + labs(x = &quot;Context X Sentence&quot;, y = &quot;RT (ms)&quot;) 建立我们的方差分析模型 我们的数据采用长格式，其中每一行都是一个观察结果。 我们还没有进行任何数据聚合。 aov_4()函数将为我们执行此操作，因为方差分析模型需要基于均值（而不是原始数据）构建。 由于数据尚未聚合，我们实际上可以使用当前格式的数据构建两个方差分析模型。 我们可以构建一个方差分析，将受试者视为随机效应（也称为 F1 方差分析），并构建另一个方差分析，将项目视为随机因素（也称为 F2 方差分析）。 在语言研究中，进行这两种类型的分析是很常见的。 逐项目 (F2) 分析使我们能够确保我们可能发现的任何影响不仅限于实验项目的子集。 首先，我们将了解如何使用aov_4()为我们的因子设计运行按主题方差分析。 语法与我们之前运行的非常相似，尽管这次您会看到我们有一个新术语Context * Sentence 。 该术语对应于两个主要效应，以及它们之间的相互作用。 它是Context + Sentence + Context:Sentence的简写。 我们还在随机效应中指定了这一点通过将na.rm设置为 TRUE，我们告诉分析忽略可能丢失数据的个别试验 - 这有效地计算了存在数据的条件均值（并忽略了其中的试验）它丢失了）。 model_subjects &lt;- aov_4(RT ~ Context * Sentence + (1 + Context * Sentence | Subject), data = factorial_data_tidied, na.rm = TRUE) ## Warning: More than one observation per design cell, aggregating data using `fun_aggregate = mean`. ## To turn off this warning, pass `fun_aggregate = mean` explicitly. 我们可以像之前一样使用anova()函数生成输出。 anova(model_subjects) ## Anova Table (Type 3 tests) ## ## Response: RT ## num Df den Df MSE F ges Pr(&gt;F) ## Context 1 59 90195 3.1767 0.0060231 0.07984 . ## Sentence 1 59 124547 0.6283 0.0016524 0.43114 ## Context:Sentence 1 59 93889 4.5967 0.0090449 0.03616 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 如果我们想生成 F2 方差分析（按项目），我们只需更改随机效应，使其按项目而不是按受试者计算。 model_items &lt;- aov_4(RT ~ Context * Sentence + (1 + Context * Sentence | Item), data = factorial_data_tidied, na.rm = TRUE) ## Warning: More than one observation per design cell, aggregating data using `fun_aggregate = mean`. ## To turn off this warning, pass `fun_aggregate = mean` explicitly. anova(model_items) ## Anova Table (Type 3 tests) ## ## Response: RT ## num Df den Df MSE F ges Pr(&gt;F) ## Context 1 27 39844 4.0013 0.0080150 0.05561 . ## Sentence 1 27 203164 0.1221 0.0012553 0.72951 ## Context:Sentence 1 27 40168 5.7687 0.0116070 0.02346 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 您可以看到 F1 和 F2 分析都产生相似的结果。 现在让我们将我们的方差分析解释为我们的随机效应。 我们将误差修正调整为none ，因为只有一些比较实际上具有理论上的意义 - 这些是我们进行同类比较的地方 - 相同类型的句子（正面或负面）前面有一个版本的上下文因素与其他因素的比较。 emmeans(model_subjects, pairwise ~ Context * Sentence, adjust = &quot;none&quot;) ## $emmeans ## Context Sentence emmean SE df lower.CL upper.CL ## Negative Negative 1474 51.1 59 1372 1576 ## Positive Negative 1628 58.9 59 1510 1746 ## Negative Positive 1595 56.5 59 1482 1708 ## Positive Positive 1579 63.9 59 1451 1707 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Negative Negative - Positive Negative -153.9 50.0 59 -3.075 0.0032 ## Negative Negative - Negative Positive -120.9 63.4 59 -1.908 0.0612 ## Negative Negative - Positive Positive -105.2 53.5 59 -1.966 0.0540 ## Positive Negative - Negative Positive 33.0 65.5 59 0.503 0.6165 ## Positive Negative - Positive Positive 48.7 57.1 59 0.852 0.3976 ## Negative Positive - Positive Positive 15.7 60.3 59 0.261 0.7953 关键的比较是Negative Negative - Positive Negative和Negative Positive - Positive Positive比较。 在第一种情况下，我们将反应时间与前面有消极与积极语境的消极句子进行比较，而在第二种情况下，我们将反应时间与前面有消极与积极语境的积极句子进行比较。 我们可以通过将相应的p值乘以 2（并对可能的最大p值限制为 1）来手动纠正多重比较（在本例中为 2）。 在这种情况下，第一个关键比较很重要 ( p = .0064)，而第二个则不重要。 我们可能会写出这样的结果： 我们进行了 2（上下文：正与负）x 2（句子：正与负）重复测量方差分析，以研究上下文效价对正价或负价句子的反应时间的影响。 方差分析显示句子没有影响 (F &lt; 1)，上下文没有影响 (F(1, 59) = 3.18, p = .080, ηG2 = .006)，但句子和上下文之间存在交互作用 (F(1, 59) = 4.60， p = .036，ηG2 = .009)。 通过进行 Bonferroni 校正的成对同伴来解释相互作用。 这些比较表明，交互是由否定句子在否定上下文中处理速度更快（1,474 毫秒 vs. 1,628 毫秒，t(118) = 3.08， p = .0064）驱动的，而正面句子的阅读速度相似在消极与积极情境中（1,595 毫秒与 1,579 毫秒，t(118) = .261， p = 1）。 作业 现在希望你自己解决以下问题。 问题1 我们的第一个数据文件名为 ANOVA_data1.csv，可以在此处找到： 24 名参与者对常见（即高词汇频率，high lexical frequency）或罕见（即低词汇频率，low lexical frequency）的单词进行了回答。 这是我们的 IV，编码为“high”与“ low”。 我们的 DV 是反应时间（reaction time），编码为“RT”。 被试编号（Subject number）编码为“Subject”。 我们想知道条件之间是否存在差异（如果有，差异在哪里）。 可视化数据、生成描述性并运行适当的方差分析以确定我们的自变量（Condition）是否对因变量 (RT) 有影响。 问题2 我们的第二个数据文件名为 ANOVA_data2.csv，可以在此处找到： 这些数据也来自反应时间实验，但设计稍微复杂一些。48 名参与者对一个词的反应频率有所不同。 这个因素是参与者之间的因素(between participants)，我们有四个级别，编码为“very low”、“low”、“high”和“very high”。 我们的 DV 是反应时间，编码为“RT”。 被试编号编码为“Subject”。 我们想知道我们的条件(condition)是否存在差异（如果有，差异在哪里）。 问题3 我们的第三个数据文件称为 ANOVA_data3.csv，可以在此处找到： 这些数据来自 2 x 2 重复测量反应时间实验。 我们感兴趣的是参与者对大图像与小图像、彩色图像与黑白图像的反应速度。 我们预计大彩色图像的响应速度将比小黑白图像的响应速度更快。 我们不确定小彩色图像和大黑白图像。 我们测量了 24 名参与者在这四种情况下对图像的反应时间。 我们想要确定我们的条件是否存在差异（如果有，差异在哪里）。 问题4 我们的第四个数据文件称为 ANOVA_data4.csv，可以在此处找到： 这些数据来自 2 x 2 x 3 混合设计实验，我们测量了人们对简单或困难的数学问题的反应时间，以及他们必须在时间压力或无时间压力下做出反应的时间。 这是我们的前两个因素，并且是重复测量（即，每个人都看到了所有 4 个条件）。 我们的第三个因素是受试者之间的因素，对应于我们的参与者是否属于三组之一。 这些小组是心理学学生、数学学生和艺术学生。 我们想知道参与者在时间压力下和没有时间压力下在数学问题上的表现受到他们属于这三个组中哪一组的影响。 进行适当的方差分析来回答这个问题。 请记住从一些数据可视化开始。 答案 问题1 方差分析应揭示为 F(1, 22) = 91.217 。 由于我们的因素只有两个水平，因此我们不需要运行任何后续测试来了解是什么推动了这种效果。 通过查看描述性统计数据，我们发现high条件下的 RT 为 865， low条件下的 RT 为 1178。 问题2 方差分析应揭示条件的影响 F(3, 44) = 203.21 。 为了进一步解释这一点，我们需要进行后续比较。 使用 Bonferroni 校正，这些应该表明每个条件级别都不同于其他级别。 问题3 方差分析应揭示Size的主效应 (F(1, 23) = 198.97)、Colour的主效应 (F(1, 23) = 524.27) 以及这两个因素之间的交互作用 (F(1, 23) = 11.08）。 为了进一步解释这种相互作用，我们需要进行后续比较。 使用 Bonferroni 校正，这些应该表明每个条件级别都不同于其他级别。 问题4 这个问题比我们迄今为止看到的问题稍微棘手一些。 构建方差分析后（请记住仅将重复测量因子添加到随机效应项中），您将发现显着的三向交互作用 (F(2, 69) = 4.63）。 您需要进一步细分 - 一种可能的方法是针对组间因素的每个级别分别查看难度和时间压力之间的相互作用。 换句话说，您将为每个学生组构建一个 2 x 2 方差分析模型。 如果这样做，您会发现 2 x 2 交互对于艺术组和数学组都不显着，但对于心理学组显着 (F(1, 23) = 11.08)） - 如下所示难度和时间压力也是两个主要影响。 但当这两个因素相互作用时，我们数据中的含义在于相互作用项（而不仅仅是这两个主效应）。 因此，三向交互告诉我们，至少有一个组的双向交互是不同的（相对于其他组）。 我们需要通过进行成对比较来为我们的心理学小组进一步检查这种双向互动，正如您在上面所做的那样。 这将揭示，对于我们的心理学小组来说，每种情况都与其他情况显着不同。 这意味着对于心理学学生来说，困难问题在时间压力下比在无时间压力下需要更长的时间，而简单问题在时间压力下比在无时间压力下需要更长的时间（但这些都没有困难问题那么长） 。 帮助我们改进本节课程 此处应插入文本和反馈二维码或问卷链接 编者注：不构成医学建议↩︎ "],["一般线性模型---方差分析第-2-部分.html", "Chapter 9 一般线性模型 - 方差分析第 2 部分 概述 建立我们的ANCOVA AN(C)OVA 作为回归的特例 帮助我们改进本节课程", " Chapter 9 一般线性模型 - 方差分析第 2 部分 在本次研讨会中，我们将在上一节的基础上探索协方差分析 (ANCOVA)。 在本次研讨会中，我们还将研究方差分析和方差分析作为回归的特殊情况，并了解如何通过线性模型构建这两者。 当你自己这样做时，你将有望确信方差分析和回归实际上是同一件事。 概述 在本次研讨会中，我们将继续检验方差分析。 具体来说，我们将重点关注 ANCOVA（协方差分析），然后再探讨 AN(C)OVA 为何是回归的特殊情况以及如何在一般线性模型的背景下理解两者。 建立我们的ANCOVA 加载我们的包 让我们看一下我在上面视频中介绍的 ANCOVA 示例。 首先我们需要加载我们需要的包。 我们将使用tidyverse进行一般数据整理和数据可视化，然后使用afex包来构建我们的 ANCOVA 模型，并使用emmeans包来运行成对比较并显示我们的调整后的平均值。 library(tidyverse) # Load the tidyverse packages library(afex) # ANOVA functions library(emmeans) # Needed for pairwise comparisons 读取我们的数据 现在我们要读入我们的数据。数据可以在这里下载 my_data &lt;- read_csv(&quot;Dataset/ancova_data.csv&quot;) head(my_data) ## # A tibble: 6 × 4 ## Participant Condition Ability Gaming ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Water 3.49 9.37 ## 2 2 Water 5.61 10.7 ## 3 3 Water 5.29 9.35 ## 4 4 Water 4.75 10.2 ## 5 5 Water 4.44 9.57 ## 6 6 Water 2.53 7.37 我们发现条件（Condition）没有正确编码为一个因素，所以让我们解决这个问题。 my_data &lt;- my_data %&gt;% mutate(Condition = factor(Condition)) head(my_data) ## # A tibble: 6 × 4 ## Participant Condition Ability Gaming ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Water 3.49 9.37 ## 2 2 Water 5.61 10.7 ## 3 3 Water 5.29 9.35 ## 4 4 Water 4.75 10.2 ## 5 5 Water 4.44 9.57 ## 6 6 Water 2.53 7.37 总结我们的数据 接下来让我们进行一些汇总统计并构建数据可视化。 my_data %&gt;% group_by(Condition) %&gt;% summarise(mean_ability = mean(Ability)) ## # A tibble: 3 × 2 ## Condition mean_ability ## &lt;fct&gt; &lt;dbl&gt; ## 1 Double Espresso 9.02 ## 2 Single Espresso 6.69 ## 3 Water 4.82 可视化我们的数据 set.seed(1234) ggplot(my_data, aes(x = Gaming, y = Ability, colour = Condition)) + geom_point(size = 3, alpha = .9) + labs(x = &quot;Gaming Frequency (hours per week)&quot;, y = &quot;Motor Ability&quot;) + theme_minimal() + theme(text = element_text(size = 11)) 我们构建了一个可视化，使用geom_point()函数绘制了原始数据点。 建立我们的方差分析模型 让我们首先构建一个方差分析模型，并忽略数据集中协变量的存在。 anova_model &lt;-aov_4(Ability ~ Condition + (1 | Participant), data = my_data) ## Contrasts set to contr.sum for the following variables: Condition anova(anova_model) ## Anova Table (Type 3 tests) ## ## Response: Ability ## num Df den Df MSE F ges Pr(&gt;F) ## Condition 2 42 1.2422 53.432 0.71786 2.882e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 根据此输出，我们似乎具有条件效果。 为了进一步探讨这一点，我们将使用emmeans()函数来运行成对比较。 在此基础上，我们可能会得出这样的结论：我们受到条件的影响，并且我们的三组中的每一组都与其他组有显着差异。 但这是对的吗？ 不，因为我们没有考虑协变量。 我们将构建我们的 ANCOVA 模型，在我们的实验条件操作之前添加我们的协变量。 我们将factorize参数设置为FALSE ，以便将其视为连续预测变量，而不是模型中的实验因子。 model_ancova &lt;- aov_4(Ability ~ Gaming + Condition + (1 | Participant), data = my_data, factorize = FALSE) ## Warning: Numerical variables NOT centered on 0 (i.e., likely bogus results): ## Gaming ## Contrasts set to contr.sum for the following variables: Condition anova(model_ancova) ## Anova Table (Type 3 tests) ## ## Response: Ability ## num Df den Df MSE F ges Pr(&gt;F) ## Gaming 1 41 0.55171 53.5636 0.56643 5.87e-09 *** ## Condition 2 41 0.55171 0.8771 0.04103 0.4236 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 在此输出的基础上，我们看到我们不再具有条件的影响，但确实具有协变量的影响。 我们可以使用emmeans()函数来生成调整后的均值（即，考虑到协变量的影响，三组中每组的均值）。 emmeans(model_ancova, pairwise ~ Condition) ## $emmeans ## Condition emmean SE df lower.CL upper.CL ## Double Espresso 6.32 0.415 41 5.48 7.16 ## Single Espresso 6.87 0.193 41 6.48 7.26 ## Water 7.33 0.393 41 6.53 8.12 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Double Espresso - Single Espresso -0.552 0.478 41 -1.155 0.4863 ## Double Espresso - Water -1.008 0.761 41 -1.324 0.3900 ## Single Espresso - Water -0.456 0.418 41 -1.092 0.5244 ## ## P value adjustment: tukey method for comparing a family of 3 estimates AN(C)OVA 作为回归的特例 我们现在将ANOVA（然后是AN(C)OVA）视为回归的一个特例。 可视化我们的数据 让我们用 x 轴上的条件(condition)来可视化数据。 my_data %&gt;% ggplot(aes(x = Condition, y = Ability, colour = Condition)) + geom_violin() + geom_jitter(width = .05, alpha = .8) + labs(x = &quot;Condition&quot;, y = &quot;Motor Ability&quot;) + stat_summary(fun.data = mean_cl_boot, colour = &quot;black&quot;) + guides(colour = FALSE) + theme_minimal() + theme(text = element_text(size = 12)) 让我们检查一下我们的条件因子当前是如何根据其对比进行编码的。 请注意，表达式my_data$Condition是引用数据集my_data中名为Condition的变量的 Base R 方式。 设置我们的对比 contrasts(my_data$Condition) ## Single Espresso Water ## Double Espresso 0 0 ## Single Espresso 1 0 ## Water 0 1 我们希望我们的水组成为参考水平（因此对应于我们的线性模型的截距），并且虚拟编码为（0, 0），但目前还没有这样编码。 让我们解决这个问题。 my_data &lt;- my_data %&gt;% mutate(Condition = fct_relevel(Condition, c(&quot;Water&quot;, &quot;Double Espresso&quot;, &quot;Single Espresso&quot;))) contrasts(my_data$Condition) ## Double Espresso Single Espresso ## Water 0 0 ## Double Espresso 1 0 ## Single Espresso 0 1 方差分析作为线性模型 好的，这就是我们现在想要的。 让我们使用lm()函数对线性模型进行建模并检查结果。 model_lm &lt;- lm(Ability ~ Condition, data = my_data) model_lm ## ## Call: ## lm(formula = Ability ~ Condition, data = my_data) ## ## Coefficients: ## (Intercept) ConditionDouble Espresso ConditionSingle Espresso ## 4.817 4.199 1.871 我们可以看到截距对应于我们的水状况的平均值。 为了计算出 Double Espresso 组的平均能力，我们使用 Double Espresso 组的编码 (1, 0) 和我们的方程： Ability = Intercept + β1(Double Espresso) + β2(Single Espresso) Ability = 4.817 + 4.199(1) + 1.871(0) Ability = 4.817 + 4.199 Ability = 9.016 为了计算单份浓缩咖啡组的平均能力，我们使用单份浓缩咖啡组的编码 (0, 1) 和我们的方程： Ability = 4.817 + 4.199(0) + 1.871(1) Ability = 4.817 + 1.871 Ability = 6.688 ANCOVA 作为线性模型 好的，现在要使用lm()函数构建我们的 ANCOVA，我们只需将协变量 ( Gaming ) 添加到我们的模型规范中。 model_ancova &lt;- lm(Ability ~ Gaming + Condition, data = my_data) model_ancova ## ## Call: ## lm(formula = Ability ~ Gaming + Condition, data = my_data) ## ## Coefficients: ## (Intercept) Gaming ConditionDouble Espresso ## -3.4498 0.8538 -1.0085 ## ConditionSingle Espresso ## -0.4563 我们可以通过将值代入方程来计算出参考组（Water）的平均值 - 请注意，游戏（Gaming）不是一个因素，我们需要输入该变量的平均值。 我们可以通过以下方法来解决。 mean(my_data$Gaming) ## [1] 12.62296 我们将此平均值 (12.62296) 与每个预测变量的系数一起添加到我们的方程中。 通过我们的虚拟编码方案，我们可以计算出水组的调整后平均值。 Ability = Intercept + β1(Gaming) + β2(Double Espresso) + β3(Single Espresso) Ability = -3.4498 + 0.8538(12.62296) + (- 1.0085)(0) + (-0.4563)(0) Ability = -3.4498 + 10.777 Ability = 7.33 7.33 是水组的调整平均值，这是我们通过调用 ANCOVA 后的emmeans()函数得到的值。 尝试使用我们的虚拟编码来计算其他两个条件的调整方法。 以协变量为中心(Centering our Covariate) 在视频中，我提到我们还可以缩放协变量并将其居中。 这使变量标准化（均值以零为中心），并且无需将协变量的线性模型系数乘以协变量的均值。 一般来说，它使我们的线性模型中的系数的解释变得更容易。 我们可以使用scale()函数在数据框中创建协变量的新（缩放和居中）版本。 my_scaled_data &lt;- my_data %&gt;% mutate(centred_gaming = scale(Gaming)) 我们可以查看非中心协变量和中心协变量，发现数据中没有任何变化，除了变量均值现在以零为中心并且分布已按比例缩放之外。 plot(density(my_scaled_data$Gaming)) plot(density(my_scaled_data$centred_gaming)) model_ancova_centred &lt;- lm(Ability ~ centred_gaming + Condition, data = my_scaled_data) model_ancova_centred ## ## Call: ## lm(formula = Ability ~ centred_gaming + Condition, data = my_scaled_data) ## ## Coefficients: ## (Intercept) centred_gaming ConditionDouble Espresso ## 7.3280 2.3046 -1.0085 ## ConditionSingle Espresso ## -0.4563 我们看到截距现在对应于水组的调整后平均值。 我们可以通过从 7.3280 减去 1.0085 来计算 Double Espresso 组的调整平均值，并且可以通过从 7.3280 减去 0.4563 来计算 Single Espresso 组的调整平均值。 希望您看到协变量的缩放和居中使得解释我们的线性模型的系数变得容易得多。 帮助我们改进本节课程 此处应插入文本和反馈二维码或问卷链接 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
